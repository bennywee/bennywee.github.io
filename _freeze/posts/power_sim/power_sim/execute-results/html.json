{
  "hash": "48f7ea91b35316ae006a8e9f2fd935d2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Power analysis by simulation\"\nauthor: \"Benjamin Wee\"\ndate: \"2025/04/13\"\nengine: knitr\ncategories:\n  - R\n  - Python\n  - Simulation\n  - Power analysis\nformat:\n  html:\n    toc: true\n---\n\n::: {.cell}\n\n:::\n\n\n# Preamble\nThis blog post is taken from notes of a write-up I did at a previous job a few years ago when I was conducting and designing A/B tests for marketing teams. It is a condensed summary of my understanding of power analysis by simulation inspired by [Nick Huntington-Klein's excellent lecture notes on the same topic](https://nickch-k.github.io/EconometricsSlides/Week_08/Power_Simulations.html). Writing things up is a useful way of structuring my thoughts and ensuring I understand what I'm reading. The original write-up I did also had a Bayesian implementation (or interpretation) of power analysis but I've omitted it since I'm not 100\\% convinced it is the correct approach for pre-experimental design or whether other simulation frameworks are more appropriate. So I've stuck with the standard frequentist approach. Anyway, hope this is useful!\n\n# Introduction\nA common task then designing experiments is to determine whether an experiment is sufficiently \"powered\". That is, conditional on our parameter constraints and available data, how likely will our experiment and models or tests reject the null hypothesis given that the null is false. This requires assumptions on effect sizes, the type of underlying data (continuous, proportions), equal or unequal sample sizes, equal or unequal variances, etc. \n\nI've been asked many times throughout my career to size up an experiment using standard frequentist tools. However, every time I return tot he exercise I get swamped with the variety of calculators and statistical tests that can be used - most only valid under very specific conditions and for relatively simple experiments. Typically in most commercial experiment designs, there are a lot of restrictions which may impact these calculations that cannot be adjusted directly.\n\nSo I turned so _simulating_ my power analysis. Simulation is a great tool for flexible estimating the impact of different parameters and assumptions. This is done by fitting many models (or running tests) on synthetic data generated from a known data generating process (DGP). The DGP can be adjusted based on our assumptions and constraints. This is especially useful when greater complexity is built into the experimental design (multiple treatments, additional co-variates, generalising the impact of multiple experiments, etc).\n\n# Setup\nI conducted an old experiment that measured the impact of a marketing campaign on product uptake for a treatment group compared to a representative control group. The specified parameters were:\n\n- Base control uptake: **$p_c = 0.35$**\n- Minimum detectable effect of treatment **$p_t = 0.36$**\n- Confidence: **($1 - \\alpha) = 0.95$**\n- Power: **($1 - \\beta) = 0.8$**\n- Type: **One sided**\n- Sample size: **n**\n\nSo assuming a base product uptake of $p_c = 35\\%$, the minimum effect to be detected at 80\\% power is 1\\% ($p_t-p_c$) with a 5\\% false positive rate under the null hypothesis.\n\nThere are two approaches to this analysis:\n\n1) Calculate power for a known n (since this size can be restricted at the very start)\n\n2) Calculate n required for a given power (more typical question that is asked)\n\nFor my use case, I was given a restricted n and had to think of different ways of determining power for asymmetric control and treatment group sizes, so I will demonstrate 1).\n\n**Typical workflow**\n\n1) Set parameters\n\n2) Simulate synthetic data given these parameters\n\n3) Fit model on data / run statistical rest\n\n4) Extract desired estimates (p values of confidence intervals, test statistics)\n\n5) Repeat 1)-4) multiple times\n\n6) Determine what proportion tests were \"statistically significant\" (power estimate)\n\n# Benchmarking\nBefore running a simulation, let's benchmark our results against a typical power analysis. Based on our parameter settings we need ~28,312 samples in both treatment and control. The goal is to replicate these results by simulation.\n\n\n::: {.panel-tabset}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\npower.prop.test(p1 = 0.35,\n                p2 = 0.36,\n                power = 0.8,\n                sig.level = 0.05,\n                alternative = 'one.sided')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample comparison of proportions power calculation \n\n              n = 28311.97\n             p1 = 0.35\n             p2 = 0.36\n      sig.level = 0.05\n          power = 0.8\n    alternative = one.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n## Python\n\n::: {.cell}\n\n```{.python .cell-code}\nimport statsmodels.stats.api as sms\n\neffect_size = sms.proportion_effectsize(0.36, 0.35)\nsms.NormalIndPower().solve_power(effect_size, power=0.8, alpha=0.05, ratio=1, alternative = \"larger\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n28311.706512937755\n```\n\n\n:::\n:::\n\n\n:::\n\n# Synthetic data simulation based on parameter settings\nFirst, set parameter values. These can be changed and re-run for other yse cases.\n\n::: {.panel-tabset}\n\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parameters\nn <- 28312*2         # Total sample size\npc <- 0.35           # Success probability in control group (inferred from domain knowledge)\npt <- 0.36           # Success probability in treatment group (minimum 'practical' effect size)\nn_sim <- 1e3         # Number of simulations to run\ntreatment_prop <- 0.5 # Proportion in treatment\none_sided <- TRUE     # True is one sided test\nside <- \"right\"       # Defaults to right sided test if one sided test. Input 'left' for left sided test\n\n# Sample size of each group\ncontrol_prop = 1 - treatment_prop # Proportion in control\nnt <- n * treatment_prop           # Treatment group size\nnc <- n * control_prop             # Control group size\n```\n:::\n\n\n## Python\n\n::: {.cell}\n\n```{.python .cell-code}\n# Parameters\nn = 28312*2         # Total sample size\npc = 0.35           # Success probability in control group (inferred from domain knowledge)\npt = 0.36           # Success probability in treatment group (minimum 'practical' effect size)\nn_sim = 1e3         # Number of simulations to run\ntreatment_prop = 0.5 # Proportion in treatment\none_sided = True     # True is one sided test\nside = \"right\"      # Right sided test\n\n# Sample size of each group\ncontrol_prop = 1 - treatment_prop # Proportion in control\nnt = int(n * treatment_prop)      # Treatment group size\nnc = int(n * control_prop)        # Control group size\n```\n:::\n\n\n:::\n\nNext, simulate bernoulli outcomes for treatment and control group based on above parameter settings. $y$ is the outcome variable (1 for product uptake, 0 for no uptake) and x is a binary indicator variable (0 for control, 1 for treatment).\n\n::: {.panel-tabset}\n\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2025)\n\n# Control group bernoulli outcomes with probability pc\nyc <- rbinom(n = nc, size = 1, prob = pc)\n\n# Treatment group bernoulli outcomes with probability pt\nyt <- rbinom(n = nt, size = 1, prob = pt)\n\n# Dummy variable 1= treatment, 0 = control.\n# Coefficient is the relative change in log odds of success if in treatment group\nxc <- rep(0, nc)\nxt <- rep(1, nt)\n\n# Bring together in a data frame\ndf <- data.frame(y = c(yc, yt), x = c(xc, xt))\n\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  y x\n1 1 0\n2 0 0\n3 0 0\n4 0 0\n5 1 0\n6 0 0\n```\n\n\n:::\n:::\n\n\n## Python\n\n::: {.cell}\n\n```{.python .cell-code}\nimport random\n\nimport numpy as np\nimport pandas as pd\n\nrandom.seed(2025)\n\n# Control group bernoulli outcomes with probability pc\nyc = np.random.binomial(n=1, p=pc, size=nc)\n\n# Treatment group bernoulli outcomes with probability pt\nyt = np.random.binomial(n=1, p=pt, size=nt)\n\n# Dummy variable 1= treatment, 0 = control.\n# Coefficient is the relative change in log odds of success if in treatment group\nxc = np.repeat(0, nc)\nxt = np.repeat(1, nt)\n\n# Bring together in a data frame\ndf = pd.DataFrame({\"y\":np.concatenate([yc,yt]),\n                   \"const\": np.repeat(1, (nc+nt)),\n                   \"x\":np.concatenate([xc,xt])})\n```\n:::\n\n\n:::\n\n# Binomial GLM (Logistic regression)\nNext, fit a logistic regression on the synthetic data and run a hypothesis test on the treatment/control dummy variable $x$. This will be compared to a two sample proportions t test.\n\nBut why regression? Well, most statistical tests and estimation procedures (t-tests, ANOVA, etc) are all cases of general linear models that can be estimated with regression. This gives us the most flexibility when considering even more complicated experimental designs (e.g. adding other x variables for pre treatment adjustment of certain demographic characteristics, stratification across multiple cohorts, etc). It also gives a clear estimation strategy instead of fumbling through the documentation of different statistical tests.\n\n$$\n\\begin{aligned}\ny_i \\sim Binomial&(n_i,p_i) \\\\\nlog \\left(\\frac{p_i}{1-p_i}\\right) &= \\beta_0 + \\bf X \\beta_1 \\\\\nH_0: \\beta_1 &= 0 \\\\\nH_1: \\beta_1 &> 0\n\\end{aligned}\n$$\n\nSo the hypothesis to test is if $\\beta_1$ is larger than 0, significant at the 5\\% level. If we were to run this experiment multiple times under the same parameter settings, we should expect to correctly reject the null in favour of the alternative hypothesis 80\\% of the time (although this does not tell us anything about the uncertainty of the effect size). \n\nFitting the model below gives a significant result. The next step is to run this exercise multiple times to get a power estimate.\n\n::: {.panel-tabset}\n\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate/fit logistic regression\nmodel <- glm(y~x, data = df, family = 'binomial')\n\n# Model results\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ x, family = \"binomial\", data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -0.61767    0.01246 -49.582  < 2e-16 ***\nx            0.05068    0.01755   2.887  0.00389 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 73742  on 56623  degrees of freedom\nResidual deviance: 73734  on 56622  degrees of freedom\nAIC: 73738\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n\n```{.r .cell-code}\n# Extract p value and divide by 2 for one sided test (Wald test statistic is asymptotically z distributed)\ncoef(summary(model))[2,4]/2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.001943298\n```\n\n\n:::\n:::\n\n\n## Python\n\n::: {.cell}\n\n```{.python .cell-code}\nimport statsmodels.api as sm\n\n# Estimate/fit logistic regression\nmodel = sm.GLM(df[[\"y\"]], df[[\"const\", \"x\"]], family=sm.families.Binomial())\nfit = model.fit()\n\n# Model results\nfit.summary()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<table class=\"simpletable\">\n<caption>Generalized Linear Model Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>  <td> 56624</td>  \n</tr>\n<tr>\n  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td> 56622</td>  \n</tr>\n<tr>\n  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     1</td>  \n</tr>\n<tr>\n  <th>Link Function:</th>         <td>Logit</td>      <th>  Scale:             </th> <td>  1.0000</td> \n</tr>\n<tr>\n  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -37013.</td> \n</tr>\n<tr>\n  <th>Date:</th>            <td>Thu, 24 Apr 2025</td> <th>  Deviance:          </th> <td>  74026.</td> \n</tr>\n<tr>\n  <th>Time:</th>                <td>14:00:50</td>     <th>  Pearson chi2:      </th> <td>5.66e+04</td> \n</tr>\n<tr>\n  <th>No. Iterations:</th>          <td>4</td>        <th>  Pseudo R-squ. (CS):</th> <td>1.096e-07</td>\n</tr>\n<tr>\n  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td>   -0.5742</td> <td>    0.012</td> <td>  -46.382</td> <td> 0.000</td> <td>   -0.598</td> <td>   -0.550</td>\n</tr>\n<tr>\n  <th>x</th>     <td>    0.0014</td> <td>    0.018</td> <td>    0.079</td> <td> 0.937</td> <td>   -0.033</td> <td>    0.036</td>\n</tr>\n</table>\n```\n\n:::\n\n```{.python .cell-code}\n# Extract p value and divide by 2 for one sided test (Wald test statistic is asymptotically z distributed)\nfit.pvalues[1]/2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.4686058467769683\n```\n\n\n:::\n:::\n\n\n:::\n\n# Simulate multiple times and calculate power\nThe below for loop will run the same step as above 1000 times.\n\n::: {.panel-tabset}\n\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\npower_sim <- function(seed) {\n  # Binomial GLM (logistic regression)\n  set.seed(seed)\n\n  # Generate synthetic data \n  # Data generating process governed by parameters pc and pt\n  # Control group bernoulli outcomes with probability pc\n  yc <- rbinom(n = nc, size = 1, prob = pc)\n\n  # Treatment group bernoulli outcomes with probability pt\n  yt <- rbinom(n = nt, size = 1, prob = pt)\n\n  # Dummy variable treatment = 1, control = 0\n  # Coefficient is the relative change in log odds of success if in treatment group\n  xc <- rep(0, nc)\n  xt <- rep(1, nt)\n\n  # Bring together in a dataframe\n  df <- data.frame(y = c(yc, yt), x = c(xc, xt))\n\n  # Fit model\n  model <- glm(y~x, data = df, family = 'binomial')\n\n  results <- data.frame(\n    test_result = NA,\n    pvalues = NA\n  )\n\n  if(one_sided == FALSE) {\n    # Extract p values, returns TRUE if less than 0.05, FALSE otherwise\n    results[\"pvalues\"] <- coef(summary(model))[2,4]\n    results[\"test_result\"] <- results[1,\"pvalues\"] < 0.05\n  } else if (one_sided == TRUE) {\n    # One sided test, halve the p-value\n    results[\"pvalues\"] <- coef(summary(model))[2,4]/2\n\n    if(side == 'right') {\n      # Ensure test statistic is greater than the null hypothesis for right sided test\n       results[\"test_result\"] <- results[1,\"pvalues\"] < 0.05 & coef(summary(model))[2,1] > 0\n    }\n    else if(side == 'left') {\n      # Test stat less than null for left sided test\n       results[\"test_result\"] <- results[1,\"pvalues\"] < 0.05 & coef(summary(model))[2,1] < 0\n    } else {\n      stop('Error: one_sided must be TRUE or FALSE')\n    }\n  }\n  return(results)\n}\n\n# Random seed grid\nset.seed(456)\nrandom_grid <- sample(1e6, n_sim)\n\n# Set up parallelism \navailable_cores = parallel::detectCores()-1\nfuture::plan(future::multisession, workers = available_cores)\n\nsim_list <- future.apply::future_Map(function(x) power_sim(seed = x), x = random_grid, future.seed=TRUE)\nsim_df <- do.call(rbind, sim_list)\n```\n:::\n\n\nFinally, calculating the power:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(sim_df[[\"test_result\"]])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.791\n```\n\n\n:::\n:::\n\n\n## Python\n\n::: {.cell}\n\n```{.python .cell-code}\nimport multiprocess as mp\n\ndef power_sim(seed):\n    np.random.seed(seed)\n    # Generate synthetic data \n    # Data generating process governed by parameters pc and pt\n    # Control group bernoulli outcomes with probability pc\n    yc = np.random.binomial(n=1, p=pc, size=nc)\n    \n    # Treatment group bernoulli outcomes with probability pt\n    yt = np.random.binomial(n=1, p=pt, size=nt)\n    \n    # Dummy variable treatment = 1, control = 0\n    # Coefficient is the relative change in log odds of success if in treatment group\n    xc = np.repeat(0, nc)\n    xt = np.repeat(1, nt)\n    \n    # Bring together in a dataframe\n    df = pd.DataFrame({\"y\":np.concatenate([yc,yt]),\n                     \"const\": np.repeat(1, (nc+nt)),\n                     \"x\":np.concatenate([xc,xt])})\n    \n    # Fit model\n    model = sm.GLM(df[[\"y\"]], df[[\"const\", \"x\"]], family=sm.families.Binomial())\n    fit = model.fit()\n    \n    if not one_sided:\n        # Extract p values, returns TRUE if less than 0.05, FALSE otherwise\n        pvalue = fit.pvalues[1]\n        test_result = pvalue < 0.05\n    elif one_sided:\n        # One sided test, halve the p-value\n        pvalue = fit.pvalues[1]/2\n        \n        if side == 'right':\n            # Ensure test statistic is greater than the null hypothesis for right sided test\n            test_result = pvalue < 0.05 and fit.params[1] > 0\n            \n        elif side == 'left':\n            # Test stat less than null for left sided test\n            test_result = pvalue < 0.05 and fit.params[1] < 0\n            \n    else:\n      Exception('Error: one_sided must be TRUE or FALSE')\n    \n    return tuple([pvalue, test_result])\n\n# Random seed grid\nrandom.seed(2025)\nrandom_grid = list(np.random.randint(0,int(1e6), int(n_sim)))\n\n# Set up parallelism \nnprocs = mp.Pool()._processes-1\npool = mp.Pool(processes=nprocs)\nresult = pool.map_async(power_sim, random_grid)\nresult_ls = result.get()\nresult_df = pd.DataFrame(result_ls)\nresult_df.columns = [\"pvalues\", \"test_result\"]\n```\n:::\n\n\nFinally, calculating the power:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.mean(result_df[[\"test_result\"]])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.807\n```\n\n\n:::\n:::\n\n\n:::\n\nGreat! The null was rejected for around 80\\% of simulations which is in line with the original power analysis. Increasing the simulation number should see this get closer to 80\\%.\n\nThe original use case of this experiment was to consider different treatment and control group sizes. The original power test analysis assumes equal, independent sample sizes in treatment and control. Instead of looking for an appropriate statistical test, I can just reset the parameters above to have 70\\% treatment and 30\\% control proportions. This enables the most flexibility when it comes to pre-analysis design since I can simulate data with other properties for the use case at hand.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
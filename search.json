[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Master’s presentation to Adelaide data science group\n\n\n\n\n\n\n\n\n\n\n\nApr 24, 2025\n\n\nBenjamin Wee\n\n\n\n\n\n\n\n\n\n\n\n\nPower analysis by simulation\n\n\n\n\n\n\nR\n\n\nPython\n\n\nSimulation\n\n\nPower analysis\n\n\n\n\n\n\n\n\n\nApr 13, 2025\n\n\nBenjamin Wee\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Python Environments\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\nBenjamin Wee\n\n\n\n\n\n\n\n\n\n\n\n\nApplying a bayesian workflow: lessons from Statistical Rethinking (I)\n\n\n\n\n\n\nBayesian statistics\n\n\nR\n\n\nPython\n\n\nStan\n\n\n\n\n\n\n\n\n\nNov 21, 2020\n\n\nBenjamin Wee\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Matplotlib - Lessons from a ggplot user\n\n\n\n\n\n\nPython\n\n\nMatplotlib\n\n\n\n\n\n\n\n\n\nApr 6, 2020\n\n\nBenjamin Wee\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/python_env_setup/python_env_setup.html",
    "href": "posts/python_env_setup/python_env_setup.html",
    "title": "Introduction to Python Environments",
    "section": "",
    "text": "This post was promised to some friendly folk in the R community who were interested in using python. It is a conceptual introduction and a basic guide to how to set up python in a way that avoids common pain points and traps which I fell into when I first started.\nFor R users, setting up python can be extremely confusing. The typical setup for R is to download R and Rstudio and away we go. When googling how to set up python as an R user (or someone who wasn’t formally trained in software engineering or computer science), we are returned with multiple tools and tutorials with no obvious place to start.\nThe first trap I fell into was installing the latest version of python and jupyterlabs then downloading packages freely for all my projects (similar to how we would install.packages() for any R package we needed). However, I learned very quickly that this would cause problems when different package versions or python versions were required across projects, leading me into dependency hell."
  },
  {
    "objectID": "posts/python_env_setup/python_env_setup.html#prerequisites",
    "href": "posts/python_env_setup/python_env_setup.html#prerequisites",
    "title": "Introduction to Python Environments",
    "section": "Prerequisites",
    "text": "Prerequisites\nSetting up and using python requires us to get comfortable with the command line. This tutorial is biased towards MacOS. While the principles of what to do are the same, the tools and commands may be different on other operating systems (commands for windows can be found in the venv documentation). So be prepared to do some work in your terminal.\nThe following walkthrough assumes you have a single version of python3 installed on your machine. Typically when starting a new project, we want to do these steps in order:\n\nCreate a project repo\nPick a python version using pyenv (optional but recommended)\nCreate virtual environment\nActivate virtual environment\nInstall packages\n\nStep 2. isn’t strictly required if you just want to get an idea of the basics - so I talk about setting this up in the appendix. However, if you end up needing multiple versions of python3 I highly recommend using pyenv to manage multiple python versions. If not feel free to ignore it for now."
  },
  {
    "objectID": "posts/python_env_setup/python_env_setup.html#virtual-environments-with-venv",
    "href": "posts/python_env_setup/python_env_setup.html#virtual-environments-with-venv",
    "title": "Introduction to Python Environments",
    "section": "Virtual environments with venv",
    "text": "Virtual environments with venv\n\nFirst, create and change into the project directory:\n\nmkdir py_project\ncd py_project\n\nThen use python to create your virtual environment. After running this command you will see a folder called proj_env, which is where all your dependnecies will live.\n\n# Create virtual env, call it proj_env (or .proj_env if you want it hidden)\npython3 -m venv proj_env\n\nActivate your virtual env (if this works you should see the name of your virtual env directory on the left hand side of your terminal prompt)\n\nsource proj_env/bin/activate\n\n\n(proj_env) benjaminwee@Benjamins-MacBook-Pro py_project %\n\nNow you can pip install packages for your project. A standard way to do this is to list out the packages you want installed in a requirements.txt file and to install them all at once (otherwise you can do it individually using pip install &lt;package_name&gt;. You can also set the specific package version as I have done for matplotlib which is something I recommend.\n\necho \"numpy\" &gt;&gt; requirements.txt\necho \"pandas\" &gt; requirements.txt\necho \"matplotlib==3.7.0\" &gt; requirements.txt\n\npip install -r requirements.txt\n\nAnd that’s it! As long as your virtual environment is activated, pip install will install packages into proj_env and will not conflict with other python environments. If you want to exit your virtual environment then run:\n\ndeactivate"
  },
  {
    "objectID": "posts/bayesian_workflow_stats_rethinking/applying_a_bayesian_workflow_pt1_post.html",
    "href": "posts/bayesian_workflow_stats_rethinking/applying_a_bayesian_workflow_pt1_post.html",
    "title": "Applying a bayesian workflow: lessons from Statistical Rethinking (I)",
    "section": "",
    "text": "I spent the last few years studying Bayesian statistics in my spare time. Most recently, I completed Richard McElreath’s Statistical Rethinking - including his 2017 lecture series and problem sets. It is rightfully one of the most popular entry level texts in bayesian statistics. I could not recommend it more highly.\nWhile I’ve gained a lot from doing problem sets and discussing course material with other people, nothing beats testing your knowledge and challenging imposter syndrome by attempting a modelling problem on your own.\nSo I decided to apply what I’ve learned so far on the Kaggle dataset: House Prices: Advanced Regression Techniques. The goal is to practise what I’ve learned in my self study journey and hopefully demonstrate some of the best practices advocated by those in the applied statistics community. This writeup was completed in R and Python (you’ll get to choose below) and Stan* for modelling.\n*Stan is a probabilistic programming language that can flexibly estimate a wide range of probabilistic and bayesian models. It uses state of the art algorithms such as Hamiltonian Monte Carlo (HMC) which allows efficient sampling of models in high dimensions without the need for conjugate priors. Stan is used across academia and industry and notably in facebook’s open source forecasting tool, prophet."
  },
  {
    "objectID": "posts/bayesian_workflow_stats_rethinking/applying_a_bayesian_workflow_pt1_post.html#thinking-about-workflow",
    "href": "posts/bayesian_workflow_stats_rethinking/applying_a_bayesian_workflow_pt1_post.html#thinking-about-workflow",
    "title": "Applying a bayesian workflow: lessons from Statistical Rethinking (I)",
    "section": "Thinking about workflow",
    "text": "Thinking about workflow\nA recurring theme in applied statistics is the importance of workflow. This topic wasn’t really covered explicitly in my econometrics/stats classes which put more emphasis on tools and derivations for predictive or causal inference. At best, components of workflow were explored in some research seminars.\nA good workflow supports quality model building. It forces us to think critically about decision making in data analysis which helps us evaluate our assumptions and identify errors. That isn’t to say there is a gold standard of how all data analysis should be conducted. Rather, following a robust methodology guides modelling decisions and helps diagnose problems. This becomes more important when adding complexity into models where it is harder to pinpoint where problems lie.\nDevelopments around workflow are a current topic of research. The most recent paper came out on November 2nd titled Bayesian Workflow which has many contributions from prominent members in the statistics community. This writeup was prepared before I had a chance to read the paper, but I hope it covers some of the principles and recommendations. And if not, like statistical models, the way I do data analysis will iterate and improve.\n\n“Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion” - Visualisation Bayesian Workflow - Gabry, Simpson, Vehtari, Betancourt, Gelman (2019)\n\nThe proposed workflow I adopted was originally inspired from blog posts by Jim Savage a few years ago and more recently, Monica Alexander’s example of visualisation in an end-to-end bayesian analysis. I’ve included a full list of resources which helped me at the end of this post. The steps in bold will be discussed below while an application using the full workflow will be in an upcoming writeup.\n\nSteps in proposed workflow\n\nExploratory data analysis and data transformation\nWrite out full probability model\nPrior predictive checks - simulate data from the implied generative model\nFit model on fake data - can we recover the known parameters?\nEstimate model on real data\nCheck whether MCMC sampler ran efficiently and model convergence\nPosterior predictive check to evaluate model fit\nModel selection using cross validation and information criteria\nOptional: Evaluate model performance on test or out of sample dataset (not strictly necessary if the modelling task is not a predictive problem)"
  },
  {
    "objectID": "posts/bayesian_workflow_stats_rethinking/applying_a_bayesian_workflow_pt1_post.html#pick-your-language",
    "href": "posts/bayesian_workflow_stats_rethinking/applying_a_bayesian_workflow_pt1_post.html#pick-your-language",
    "title": "Applying a bayesian workflow: lessons from Statistical Rethinking (I)",
    "section": "Pick your language",
    "text": "Pick your language\n\nPythonR\n\n\n\n1) Exploratory data analysis and data transformation\nThe full dataset for this competition contains 79 features to predict the target variable SalesPrice. For this exercise I will focus on two variables: Neighbourhood (categorical: physical locations within Ames city limits) and LotArea (positive real: lot size in square feet). I chose these variables as they are consistent with my understanding of how housing prices vary in relation to their location and property size.\nAside: The model and feature selection in this example are deliberately simple. The goal is to motivate workflow, diagnostics and to interrogate assumptions, so I only used two variables to make it easier to follow. My repo contains examples of other models and additional features.\n\nimport pystan\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport arviz as az\nimport math\n\naz.style.use('arviz-darkgrid')\n\ndf = pd.read_csv('data/train.csv').loc[:, ['SalePrice', 'LotArea', 'Neighborhood']]\n\n# Log transform\ndf['log_sales_price'] = np.log(df['SalePrice'])\ndf['log_lot_area'] = np.log(df['LotArea'])\n\n# Create numerical categories (add 1 due to zero indexing)\ndf['neighbourhood'] = df['Neighborhood'].astype('category').cat.codes+1\n\ndf.head().style\n\n\n\n\n\n\n\n\n \nSalePrice\nLotArea\nNeighborhood\nlog_sales_price\nlog_lot_area\nneighbourhood\n\n\n\n\n0\n208500\n8450\nCollgCr\n12.247694\n9.041922\n6\n\n\n1\n181500\n9600\nVeenker\n12.109011\n9.169518\n25\n\n\n2\n223500\n11250\nCollgCr\n12.317167\n9.328123\n6\n\n\n3\n140000\n9550\nCrawfor\n11.849398\n9.164296\n7\n\n\n4\n250000\n14260\nNoRidge\n12.429216\n9.565214\n16\n\n\n\n\n\n\n\nA scatter plot shows a positive correlation between log(SalePrice) and log(LotArea). Fitting OLS on the logarithms of both variables assumes a linear relationship on the multiplicative scale. All else equal, property prices tend to be higher with larger lot sizes. However, this univariate linear model clearly underfits the data and there are almost surely unobserved confounding variables.\n\nsns.lmplot(x='log_lot_area',y='log_sales_price',data=df,fit_reg=True, ci = False)\nplt.tight_layout()\n\n\n\n\nA potential reason for underfitting may be some neighbourhoods have higher average prices than other neighbourhoods (which would result in different intercepts). Furthermore, the association between housing prices and lot size may depend on different neighbourhoods as well (varying slopes). This variation could be driven by different zonings or housing densities within neighbourhoods that could impact the relationship between lot size and prices. Splitting the plot out by neighbourhood displays the heterogeneity in linear trends.\n\nfacet_scatter = sns.lmplot(x=\"log_lot_area\", \n                           y=\"log_sales_price\", \n                           col=\"Neighborhood\",\n                           col_wrap = 5,\n                           data=df,\n                           ci = None, \n                           truncate=False,\n                           col_order = sorted(df['Neighborhood'].drop_duplicates()))\n\nfacet_scatter.set(xlim=(5, 15))\nfacet_scatter.set(ylim=(5, 15))\n\n\n\n\nWe can see variation in the slopes and intercepts as well as imbalanced sampling between neighbourhood clusters. This and other unobserved confounders probably contributed to some of the weak/negative gradients. The small sample sizes in some neighbourhoods will be prone to overfitting and will give noisy estimates which will require regularisation.\n\n\n2) Write out full probability model\n3 basic linear models can be used to approach this problem:\n\nPooled OLS (assumes all observations come from “one neighbourhood”, equivalent to the OLS model in the first scatterplot)\nNo pooling OLS (conceptually the same as a dummy variable regression - assumes independence between all neighbourhoods)\nSaturated regression (adds interactive effects between log(LotArea)_i and neighbourhood to no pooling OLS)\n\nI will use no pooling OLS to demonstrate the rest of the workflow. There is definitely room for improving these models. In fact, this problem is a good candidate for multilevel models. They allow for information to be borrowed between neighbourhood clusters through partial pooling and removes the assumption of independence. This will help regularise the effects of small and imbalanced sample sizes across neighbourhood. I will apply the full workflow using multilevel models in the next post.\n\nModel specification\nThe no pooling regression is written out below, where \\(i\\) indexes the property and \\(j\\) indexes each neighbourhood. I’ve assigned a gaussian likelihood which assumes that the residuals are normally distributed.\n\\[\n\\begin{align}\ny_i &\\sim Normal(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_{j} + \\beta * x_i \\\\\n\\end{align}\n\\] Where \\(y_i\\) is log(SalesPrice) and \\(x_i\\) is log(LotArea) scaled to mean 0 and standard deviation 1. \\(\\alpha_j\\) is an intercept parameter for the jth neighbourhood in the sample. The slope coefficient can be interpreted as: a one standard deviation increase in log(LotArea) is a \\(\\beta\\) standard deviation change in log(SalesPrice).\n\n# Standardise predictors\ndef z_std(x):\n    \"\"\"\n    Centres at mean 0 and standard deviation 1\n    \"\"\"\n    z_score = (x - x.mean()) / x.std()\n    return(z_score)\n    \n# Center and scale predictor\ndf['log_lot_area_z'] = z_std(df['log_lot_area'])\n\n# Scale target\ndf['log_sales_price_z'] = z_std(df['log_sales_price'])\n\n\\[\n\\begin{align}\ny_i &= \\frac{log(SalesPrice)_i - \\overline{log(SalesPrice)}}{\\sigma_{log(SalesPrice)}} \\\\\nx_i &= \\frac{log(LotArea)_i - \\overline{log(LotArea)}}{\\sigma_{log(LotArea)}}\n\\end{align}\n\\]\nStandardising both outcome and predictor variables makes sampling from the posterior distribution easier when we fit the model. If we had more continuous regressors, we could also compare the parameters on the same scale. Standardising also plays an important role in setting priors as we’ll see below.\n\n\nSelecting priors\nProbability distributions need to be assigned to the parameters for this to be a bayesian model. Setting priors is an opportunity to encode domain knowledge or results from related studies into the model. Unfortunately, I do not have much domain expertise or information about the context of this dataset to give very informative priors. So I have chosen to use weakly informative priors following the advice of the Stan developers. This will help me regularise model predictions within the plausible outcome space.\nFor \\(\\beta\\) I’ll assign a \\(Normal(0, 1)\\) which puts ~95% of the probability between two standard deviations for a unit increase in \\(x\\). We want to hedge against overfitting by shrinking the coefficient towards zero. This is achieved by putting probability mass on all plausible values of \\(\\beta\\) with less weight on extreme relationships.\n\\(\\alpha_j\\) is the intercept for the \\(j^{th}\\) neighbourhood. In a pooled OLS regression between price and lot area, the intercept \\(\\alpha\\) (ignoring the neighbourhood means ignoring the j subscript) would be interpreted as the value of \\(y\\) when \\(x\\) is 0. Since \\(x\\) has a mean of zero, \\(\\alpha\\) has the additional interpretation as the value of \\(y\\) when \\(x\\) is equal to its sample mean. By construction, \\(\\alpha\\) must be 0, the sample mean of \\(y\\).\nSo in the case of \\(\\alpha_j\\) I set a normal prior with a mean of 0 and a standard deviation of 1 for all neighbourhoods, regularising neighbourhood effects within two standard deviations of the grand mean of \\(y\\).\nThe variance parameter \\(\\sigma\\) is defined over positive real numbers. So our prior should only put probabilistic weight on positive values. In this case I’ve chosen a weakly regularising \\(exponential(1)\\) prior. Other candidate priors are the Half-Cauchy distribution or the Half-Normal which has thinner tails.\nThese weakly informative priors express my belief that the parameters of this model would overfit the sample and that we need to regularise their effects. Standardising the variables made this job much easier and intuitive. All together the full model looks like:\n\\[\n\\begin{align}\ny_i &\\sim Normal(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_{j} + \\beta * x_i \\\\\n\\alpha_j &\\sim Normal(0, 1)\\\\\n\\beta &\\sim Normal(0, 1) \\\\\n\\sigma &\\sim exp(1)\n\\end{align}\n\\]\n\n\n\n3) Prior predictive checks - simulate data from the implied generative model\nPrior predictive checks are useful for understanding the implications of our priors. Parameters are simulated from the joint prior distribution and visualised to see the implied relationships between the target and predictor variables. This will help diagnose any problems with our assumptions and modelling decisions. These checks become more important for generalised linear models since the outcome and parameter space are different due to the link function. For example, priors on parameters in a logistic regression are in the log-odds space and may behave differently to our expectations on the outcome space.\nThe code below includes all the inputs necessary to estimate the model on the data. Setting run_estimation = 0 means Stan will only simulate values from the joint prior distribution since the likelihood is not evaluated (thanks to Jim for this handy tip).\n\nno_pooling_stan_code = '''\n// No pooling model for predicting housing prices\ndata {\n    // Fitting the model on training data\n    int&lt;lower=0&gt; N; // Number of rows\n    int&lt;lower=0&gt; neighbourhood[N]; // neighbourhood categorical variable\n    int&lt;lower=0&gt; N_neighbourhood; // number of neighbourhood categories\n    vector[N] log_sales_price; // log sales price\n    vector[N] log_lot_area; // log lot area\n\n    // Adjust scale parameters in python\n    real alpha_sd;\n    real beta_sd;\n    \n    // Set to zero for prior predictive checks, set to one to evaluate likelihood\n    int&lt;lower = 0, upper = 1&gt; run_estimation;\n}\nparameters {\n    vector[N_neighbourhood] alpha; // Vector of alpha coefficients for each neighbourhood\n    real beta;\n    real&lt;lower=0&gt; sigma;\n}\nmodel {\n    // Priors\n    target += normal_lpdf(alpha | 0, alpha_sd);\n    target += normal_lpdf(beta | 0, beta_sd);\n    target += exponential_lpdf(sigma |1);\n    \n    // Likelihood\n    if(run_estimation==1){\n        target += normal_lpdf(log_sales_price | alpha[neighbourhood] + beta * log_lot_area, sigma);\n\n    }\n}\ngenerated quantities {\n    // Uses fitted model to generate values of interest\n    vector[N] log_lik; // Log likelihood\n    vector[N] y_hat; // Predictions using training data\n    {\n    for(n in 1:N){\n          log_lik[n] = normal_lpdf(log_sales_price | alpha[neighbourhood[n]] + beta * log_lot_area[n], sigma);\n          y_hat[n] = normal_rng(alpha[neighbourhood[n]] + beta * log_lot_area[n], sigma);      \n        }\n    }\n}\n'''\n\n# Dictionary contains all data inputs\nnpm_data_check = dict(N = len(df),\n                      log_sales_price = df['log_sales_price_z'],\n                      log_lot_area = df['log_lot_area_z'],\n                      neighbourhood = df['neighbourhood'],\n                      N_neighbourhood = len(df['neighbourhood'].unique()),\n                      alpha_sd = 1, \n                      beta_sd = 1, \n                      run_estimation = 0)\n\n# Compile stan model\nno_pooling_model = pystan.StanModel(model_code = no_pooling_stan_code)\n\n# Draw samples from joint prior distribution\nfit_npm_check = no_pooling_model.sampling(data=npm_data_check, seed = 12345)\n\n# Extract samples into a pandas dataframe\nnpm_df_check = fit_npm_check.to_dataframe()\n\n\nFor the prior predictive checks, we recommend not cleaving too closely to the observed data and instead aiming for a prior data generating process that can produce plausible data sets, not necessarily ones that are indistinguishable from observed data. - Visualisation Bayesian Workflow - Gabry, Simpson, Vehtari, Betancourt, Gelman (2019)\n\nThe implied predictions of our priors are visualised below. I’ve arbitrarily chosen the 4th neighbourhood index (\\(\\alpha_{j=4}\\)) since the priors for the neighbourhoods are the same. Weakly informative priors should create bounds between possible values while allowing for some implausible relationships. Remembering that 95% of gaussian mass exists within two standard deviations of the mean is a useful guide for determining what is reasonable.\nLet’s see an example of setting uninformative priors and its implications of the data generating process. I’ve set the scale parameters for \\(\\alpha\\) and \\(\\beta\\) to be 10 which are quite diffuse. The implied predictions of the mean are much wider and well beyond the minimum and maximum values in the real data. This suggests that the model is giving too much probabilistic weight to highly implausible datasets.\n\nnpm_data_check_wide = dict(N = len(df),\n                      log_sales_price = df['log_sales_price_z'],\n                      log_lot_area = df['log_lot_area_z'],\n                      neighbourhood = df['neighbourhood'],\n                      N_neighbourhood = len(df['Neighborhood'].unique()),\n                      alpha_sd = 10, \n                      beta_sd = 10, \n                      run_estimation = 0)\n\nfit_npm_check_wide = no_pooling_model.sampling(data=npm_data_check_wide)\nnpm_df_check_wide = fit_npm_check_wide.to_dataframe()\n\n_, ax = plt.subplots(figsize = (13, 8))\n\nx = np.linspace(-3, 3, 200)\n\nfor alpha, beta in zip(npm_df_check_wide[\"alpha[4]\"][:100], npm_df_check_wide[\"beta\"][:100]):\n    y = alpha + beta * x\n    ax.plot(x, y, c=\"k\", alpha=0.4)\n\nax.set_xlabel(\"x (z-scores)\")\nax.set_ylabel(\"Fitted y (z-scores)\")\nax.set_title(\"Prior predictive checks -- Uninformative (flat) priors\");\n\n\n\n\nOur original scale parameters of 1 produce more reasonable relationships. There are still some extreme regression lines implied by our data generating process, but they are bound to more realistic outcomes relative to the diffuse priors.\n\n_, ax = plt.subplots(figsize = (13, 8))\n\nx = np.linspace(-3, 3, 200)\n\nfor alpha, beta in zip(npm_df_check[\"alpha[4]\"][:100], npm_df_check[\"beta\"][:100]):\n    y = alpha + beta * x\n    ax.plot(x, y, c=\"blue\", alpha=0.4)\n\nax.set_xlabel(\"x (z-scores)\")\nax.set_ylabel(\"Fitted y (z-scores)\")\nax.set_title(\"Prior predictive checks -- Weakly regularizing priors\")\n\n\n\n\nPutting both sets of lines on the same scale emphasises the difference in simulated values. The blue lines from the previous graph cover a tighter space relative to the simulations from the uninformative priors.\n\n# Putting both on the same scale\n_, ax = plt.subplots(figsize = (13, 8))\n\nfor alpha_wide, beta_wide in zip(npm_df_check_wide[\"alpha[4]\"][:100], npm_df_check_wide[\"beta\"][:100]):\n    y_wide = alpha_wide + beta_wide * x\n    ax.plot(x, y_wide, c=\"k\", alpha=0.4)\n    \nfor alpha, beta in zip(npm_df_check[\"alpha[4]\"][:100], npm_df_check[\"beta\"][:100]):\n    y = alpha + beta * x\n    ax.plot(x, y, c=\"blue\", alpha=0.2)\n\nax.set_xlabel(\"x (z-scores)\")\nax.set_ylabel(\"Fitted y (z-scores)\")\nax.set_title(\"Prior predictive checks -- Uninformative (black) vs weakly informative (blue)\")\n\n\n\n\n\n\n4) Fit model on fake data\nWe can use the simulations to see if our model can successfully estimate the parameters used to generate fake data (the implied \\(\\hat{y}\\)). Take a draw from the prior samples (e.g. the 50th simulation) and estimate the model on the data produced by these parameters. Let’s see if the model fitted on fake data can capture the “true” parameters (dotted red lines) of the data generating process. If the model cannot capture the known parameters which generated fake data, there is no certainty it will be estimating the correct parameters on real data.\n\n# Pick random simulation, let's say 50\nrandom_draw = 50\n\n# Extract the simulated (fake) data implied by the parameters in sample 50\ny_sim = npm_df_check.filter(regex = 'y_hat').iloc[random_draw, :]\n\n# Extract the parameters corresponding to sample 10\ntrue_parameters = npm_df_check.filter(regex = 'alpha|beta|sigma').iloc[random_draw, :]\n\n# Fit the model on the fake data\n_npm_data_check = dict(N = len(df),\n              log_sales_price = y_sim, # this is now fitting on the extracted fake data in sample 50\n              log_lot_area = df['log_lot_area_z'],\n              neighbourhood = df['neighbourhood'],\n              N_neighbourhood = len(df['Neighborhood'].unique()),\n              alpha_sd = 1, \n              beta_sd = 1, \n              run_estimation = 1)\n\n_fit_npm_check = no_pooling_model.sampling(data=_npm_data_check, seed = 12345)\n_npm_df_check = _fit_npm_check.to_dataframe()\nfake_fit = _npm_df_check.filter(regex = 'alpha|beta|sigma')\nparameter_df = pd.melt(fake_fit)\n\n# Plot will give distributions of all parameters to see if it can capture the known parameters\nfig, axes = plt.subplots(nrows=max(2, math.ceil(fake_fit.shape[1] / 6)), ncols=6, sharex=False, sharey = False, figsize=(21,13))\nfig.suptitle('Model Checking - red lines are \"true\" parameters', size = 30)\naxes_list = [item for sublist in axes for item in sublist] \nparameters = parameter_df[['variable']].drop_duplicates().set_index('variable').index\ngrouped = parameter_df.groupby(\"variable\")\n\nfor parameter in parameters:\n    selection = grouped.get_group(parameter)\n    ax = axes_list.pop(0)\n    selection.plot.kde(label=parameter, ax=ax, legend=False)\n    ax.set_title(parameter)\n    ax.grid(linewidth=0.25)\n    ax.axvline(x=true_parameters[parameter], color='red', linestyle='--', alpha = 0.5)\n\n# Now use the matplotlib .remove() method to delete anything we didn't use\nfor ax in axes_list:\n    ax.remove()\n\nplt.tight_layout()\n\n\n\n\nThe model sufficiently captured the known parameters. The next post will go through a more interesting example where this fails and requires us to rethink how we specified our models.\n\n\n5) Estimate model on real data\nSet run_estimation=1 and run the code to fit the model. Stan will sample the joint posterior distribution using the default Markov chain Monte Carlo (MCMC) algorithm, the No-U-Turn sampler (NUTs).\n\n# Dictionary with data inputs - set run_estimation=1\nnpm_data = dict(N = len(df),\n              log_sales_price = df['log_sales_price_z'],\n              log_lot_area = df['log_lot_area_z'],\n              neighbourhood = df['neighbourhood'],\n              N_neighbourhood = len(df['Neighborhood'].unique()),\n              alpha_sd = 1, \n              beta_sd = 1, \n              run_estimation = 1)\n\n# Fit model by sampling from posterior distribution\nfit_npm = no_pooling_model.sampling(data=npm_data)\n\n# For generating visualisations using the arviz package\nnpm_az = az.from_pystan(\n    posterior=fit_npm,\n    posterior_predictive=\"y_hat\",\n    observed_data=\"log_sales_price\",\n    log_likelihood='log_lik',\n)\n\n# Extract samples into dataframe\nfit_npm_df = fit_npm.to_dataframe()\n\n\n\n6) Check whether MCMC sampler and model fit\nStan won’t have trouble sampling from such a simple model, so I won’t go through chain diagnostics in detail. I’ve included number of effective samples and Rhat diagnostics for completeness. We can see the posterior distributions of all the parameters by looking at the traceplot as well.\n\nTraceplot\n\n# Inspect model fit\naz.plot_trace(fit_npm, \n              var_names=[\"alpha\", \"beta\", \"sigma\"], \n              compact = True, \n              chain_prop = 'color')\n\n\n\n\n\n\nPosterior distributions\n\n# Inspect model fit\naxes = az.plot_forest(fit_npm, \n              var_names=[\"alpha\", \"beta\", \"sigma\"],\n              combined = True)\n\naxes[0].set_title('Posterior distributions of fitted parameters')\n\n\n\n\n\n\nneff / Rhat\n\nprint(pystan.stansummary(fit_npm, \n                         pars=['alpha', 'beta', 'sigma'], \n                         probs=(0.025, 0.50, 0.975), \n                         digits_summary=3))\n\nInference for Stan model: anon_model_9d4f76eb27d91c6b75464a26e0b032c7.\n4 chains, each with iter=2000; warmup=1000; thin=1;\npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n            mean se_mean     sd   2.5%    50%  97.5%  n_eff   Rhat\nalpha[1]   1.004   0.002  0.148   0.71  1.004  1.288   6313  0.999\nalpha[2]   0.565   0.005  0.401 -0.211  0.564  1.334   6117  0.999\nalpha[3]  -0.102   0.002  0.162 -0.412 -0.102  0.212   4685    1.0\nalpha[4]  -0.686 9.71e-4  0.082 -0.845 -0.686 -0.523   7060    1.0\nalpha[5]   0.003   0.002  0.121 -0.2292.25e-4  0.245   6299    1.0\nalpha[6]    0.33 5.69e-4  0.051  0.229   0.33  0.429   7894  0.999\nalpha[7]    0.34   0.001  0.087  0.169  0.339  0.507   7475    1.0\nalpha[8]   -0.78 6.89e-4  0.059 -0.895 -0.781 -0.664   7357    1.0\nalpha[9]   0.215 9.01e-4  0.068  0.082  0.215   0.35   5743  0.999\nalpha[10] -1.328   0.001  0.101 -1.525 -1.329 -1.132   7365    1.0\nalpha[11]  -0.41   0.002  0.159 -0.715 -0.408 -0.105   5670    1.0\nalpha[12] -0.319 9.71e-4  0.087 -0.496  -0.32 -0.142   8023  0.999\nalpha[13]  -0.44 4.84e-4  0.041  -0.52  -0.44 -0.362   7346  0.999\nalpha[14]  0.312   0.002  0.202 -0.087  0.313  0.714   7154  0.999\nalpha[15]    0.1 8.56e-4  0.071  -0.04    0.1   0.24   6834  0.999\nalpha[16]   1.37   0.001  0.095  1.181  1.369  1.561   6408    1.0\nalpha[17]  1.412 8.08e-4  0.068  1.277  1.412  1.546   7174  0.999\nalpha[18] -0.685 6.69e-4  0.057 -0.797 -0.685  -0.57   7138  0.999\nalpha[19] -0.362   0.001  0.122 -0.598 -0.362 -0.127   7838    1.0\nalpha[20] -0.597 7.55e-4  0.071 -0.733 -0.597 -0.456   8863  0.999\nalpha[21]  0.121  9.7e-4  0.079 -0.036   0.12  0.274   6678  0.999\nalpha[22]  0.869 7.96e-4  0.066  0.739  0.869  0.995   6790    1.0\nalpha[23]   1.42   0.001  0.123  1.176  1.421  1.659   7563    1.0\nalpha[24]  0.503   0.001  0.099   0.31  0.501  0.699   7092    1.0\nalpha[25]  0.515   0.002  0.179  0.167  0.515  0.868   7366  0.999\nbeta       0.347 3.71e-4  0.021  0.307  0.347  0.387   3126  0.999\nsigma      0.607 1.37e-4  0.011  0.586  0.607  0.629   6687  0.999\n\nSamples were drawn using NUTS at Thu Nov 12 11:48:18 2020.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\n\n7) Posterior predictive check to evaluate model performance\nHow well did the model perform? We can perform posterior predictive checks to see if sampled distributions from the fitted model can approximate the density of SalesPrice. If the model performs well, it should be able to retrodict the density of the data used to train the model. The blue lines are the predictions drawn from the joint posterior distribution compared with the observed density of the target \\(y\\) variable.\n\n# Select 300 samples to plot against observed distribution\naz.plot_ppc(data = npm_az, \n            kind = 'kde', \n            data_pairs = {'log_sales_price' : 'y_hat'},\n            legend = True,\n            color='cyan',\n            mean = False,\n            figsize=(8, 5),\n            alpha = 0.5,\n            num_pp_samples=300)\n\n\n\n\nReversing the data transformations gives back the posterior predictive checks on the natural scale (rescale \\(y\\) and exponentiate log(SalesPrice) to get back SalesPrice):\n\nfig, axes = pl#| t.subplots(1,1, figsize = (13, 8))\nnp.exp(fit_npm_df.filter(regex = 'y_hat')*df['log_sales_price'].std()+df['log_sales_price'].mean())\\\n                 .T\\\n                 .iloc[:, :300]\\\n                 .plot.kde(legend = False, \n                           title = 'Posterior predictive Checks - Black: Observed Sale Price, blue: posterior samples', \n                           xlim = (30000,500000),\n                           alpha = 0.08,\n                           ax = axes, color = 'aqua');\n\ndf['SalePrice'].plot.kde(legend = False, \n                         xlim = (30000,500000),\n                         alpha = 1,\n                         ax = axes,\n                         color = 'black');\n\n\n\n\nNot bad for a simple model. There is definitely room for iteration and improvement.\n\n\n\n\n1) Exploratory data analysis and data transformation\nThe full dataset for this competition contains 79 features to predict the target variable SalesPrice. For this exercise I will focus on two variables: Neighbourhood (categorical: physical locations within Ames city limits) and LotArea (positive real: lot size in square feet). I chose these variables as they are consistent with my understanding of how housing prices vary in relation to their location and property size.\nAside: The model and feature selection in this example are deliberately simple. The goal is to motivate workflow, diagnostics and to interrogate assumptions, so I only used two variables to make it easier to follow. My repo contains examples of other models and additional features.\n\nlibrary(tidyverse)\nlibrary(rstan)\nlibrary(bayesplot)\n\n# Detect cores for parallel sampling\noptions(mc.cores = parallel::detectCores())\n\n# Load data, select variables, apply log transformation\ndf &lt;-  read_csv('data/train.csv') %&gt;% \n  select('SalePrice', 'LotArea', 'Neighborhood')%&gt;% \n  mutate(log_sales_price = log(SalePrice),\n         log_lot_area = log(LotArea),\n         neighbourhood = as.integer(as.factor(Neighborhood)))\nhead(df)\n\nA scatter plot shows a positive correlation between log(SalePrice) and log(LotArea). Fitting OLS on the logarithms of both variables assumes a linear relationship on the multiplicative scale. All else equal, property prices tend to be higher with larger lot sizes. However, this univariate linear model clearly underfits the data and there are almost surely unobserved confounding variables.\n\nggplot(df, aes(x = log_lot_area, y = log_sales_price)) +\n  geom_point(colour = 'blue') +\n  geom_smooth(method = lm, se = FALSE, formula = 'y ~ x') + \n  ggsave('figures/2r_pooling_scatter.png', dpi = 300, width=10, height = 8, units = 'in')\n\n\n\n\nA potential reason for underfitting may be some neighbourhoods have higher average prices than other neighbourhoods (which would result in different intercepts). Furthermore, the association between housing prices and lot size may depend on different neighbourhoods as well (varying slopes). This variation could be driven by different zonings or housing densities within neighbourhoods that could impact the relationship between lot size and prices. Splitting the plot out by neighbourhood displays the heterogeneity in linear trends.\n\nggplot(df, aes(x = log_lot_area, y = log_sales_price)) +\n  geom_point(colour = 'blue') +\n  geom_smooth(method = lm, se = FALSE, formula = 'y ~ x', fullrange = TRUE) +\n  facet_wrap(~Neighborhood) +\n  theme(strip.background = element_blank())\n\n\n\n\nWe can see variation in the slopes and intercepts as well as imbalanced sampling between neighbourhood clusters. This and other unobserved confounders probably contributed to some of the weak/negative gradients. The small sample sizes in some neighbourhoods will be prone to overfitting and will give noisy estimates which will require regularisation.\n\n\n2) Write out full probability model\n3 basic linear models can be used to approach this problem:\n\nPooled OLS (assumes all observations come from “one neighbourhood”, equivalent to the OLS model in the first scatterplot)\nNo pooling OLS (conceptually the same as a dummy variable regression - assumes independence between all neighbourhoods)\nSaturated regression (adds interactive effects between log(LotArea)_i and neighbourhood to no pooling OLS)\n\nI will use no pooling OLS to demonstrate the rest of the workflow. There is definitely room for improving these models. In fact, this problem is a good candidate for multilevel models. They allow for information to be borrowed between neighbourhood clusters through partial pooling and removes the assumption of independence. This will help regularise the effects of small and imbalanced sample sizes across neighbourhood. I will apply the full workflow using multilevel models in the next post.\n\n\nModel specification\nThe no pooling regression is written out below, where \\(i\\) indexes the property and \\(j\\) indexes each neighbourhood. I’ve assigned a gaussian likelihood which assumes that the residuals are normally distributed.\n\\[\n\\begin{align}\ny_i &\\sim Normal(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_{j} + \\beta * x_i \\\\\n\\end{align}\n\\] Where \\(y_i\\) is log(SalesPrice) and \\(x_i\\) is log(LotArea) scaled to mean 0 and standard deviation 1. \\(\\alpha_j\\) is an intercept parameter for the jth neighbourhood in the sample. The slope coefficient can be interpreted as: a one standard deviation increase in log(LotArea) is a \\(\\beta\\) standard deviation change in log(SalesPrice).\n\ndf  &lt;-  df %&gt;% mutate(log_lot_area_z = scale(log_lot_area),\n                    log_sales_price_z = scale(log_sales_price))\n\n\\[\n\\begin{align}\ny_i &= \\frac{log(SalesPrice)_i - \\overline{log(SalesPrice)}}{\\sigma_{log(SalesPrice)}} \\\\\nx_i &= \\frac{log(LotArea)_i - \\overline{log(LotArea)}}{\\sigma_{log(LotArea)}}\n\\end{align}\n\\] Standardising both outcome and predictor variables makes sampling from the posterior distribution easier when we fit the model. If we had more continuous regressors, we could also compare the parameters on the same scale. Standardising also plays an important role in setting priors as we’ll see below.\n\n\nSelecting priors\nProbability distributions need to be assigned to the parameters for this to be a bayesian model. Setting priors is an opportunity to encode domain knowledge or results from related studies into the model. Unfortunately, I do not have much domain expertise or information about the context of this dataset to give very informative priors. So I have chosen to use weakly informative priors following the advice of the Stan developers. This will help me regularise model predictions within the plausible outcome space.\nFor \\(\\beta\\) I’ll assign a \\(Normal(0, 1)\\) which puts ~95% of the probability between two standard deviations for a unit increase in \\(x\\). We want to hedge against overfitting by shrinking the coefficient towards zero. This is achieved by putting probability mass on all plausible values of \\(\\beta\\) with less weight on extreme relationships.\n\\(\\alpha_j\\) is the intercept for the \\(j^{th}\\) neighbourhood. In a pooled OLS regression between price and lot area, the intercept \\(\\alpha\\) (ignoring the neighbourhood means ignoring the j subscript) would be interpreted as the value of \\(y\\) when \\(x\\) is 0. Since \\(x\\) has a mean of zero, \\(\\alpha\\) has the additional interpretation as the value of \\(y\\) when \\(x\\) is equal to its sample mean. By construction, \\(\\alpha\\) must be 0, the sample mean of \\(y\\).\nSo in the case of \\(\\alpha_j\\) I set a normal prior with a mean of 0 and a standard deviation of 1 for all neighbourhoods, regularising neighbourhood effects within two standard deviations of the grand mean of \\(y\\).\nThe variance parameter \\(\\sigma\\) is defined over positive real numbers. So our prior should only put probabilistic weight on positive values. In this case I’ve chosen a weakly regularising \\(exponential(1)\\) prior. Other candidate priors are the Half-Cauchy distribution or the Half-Normal which has thinner tails.\nThese weakly informative priors express my belief that the parameters of this model would overfit the sample and that we need to regularise their effects. Standardising the variables made this job much easier and intuitive. All together the full model looks like:\n\\[\n\\begin{align}\ny_i &\\sim Normal(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_{j} + \\beta * x_i \\\\\n\\alpha_j &\\sim Normal(0, 1)\\\\\n\\beta &\\sim Normal(0, 1) \\\\\n\\sigma &\\sim exp(1)\n\\end{align}\n\\]\n\n\n3) Prior predictive checks - simulate fake data from the implied generative model\nPrior predictive checks are useful for understanding the implications of our priors. Parameters are simulated from the joint prior distribution and visualised to see the implied relationships between the target and predictor variables. This will help diagnose any problems with our assumptions and modelling decisions. These checks become more important for generalised linear models since the outcome and parameter space are different due to the link function. For example, priors on parameters in a logistic regression are in the log-odds space and may behave differently to our expectations on the outcome space.\nThe code below includes all the inputs necessary to estimate the model on the data. Setting run_estimation = 0 means Stan will only simulate values from the joint prior distribution since the likelihood is not evaluated (thanks to Jim for this handy tip).\n\nno_pooling_stan_code = \"\n// No pooling model for predicting housing prices\ndata {\n    // Fitting the model on training data\n    int&lt;lower=0&gt; N; // Number of rows\n    int&lt;lower=0&gt; neighbourhood[N]; // neighbourhood categorical variable\n    int&lt;lower=0&gt; N_neighbourhood; // number of neighbourhood categories\n    vector[N] log_sales_price; // log sales price\n    vector[N] log_lot_area; // log lot area\n\n    // Adjust scale parameters in python\n    real alpha_sd;\n    real beta_sd;\n    \n    // Set to zero for prior predictive checks, set to one to evaluate likelihood\n    int&lt;lower = 0, upper = 1&gt; run_estimation;\n}\nparameters {\n    vector[N_neighbourhood] alpha; // Vector of alpha coefficients for each neighbourhood\n    real beta;\n    real&lt;lower=0&gt; sigma;\n}\nmodel {\n    // Priors\n    target += normal_lpdf(alpha | 0, alpha_sd);\n    target += normal_lpdf(beta | 0, beta_sd);\n    target += exponential_lpdf(sigma |1);\n    //target += normal_lpdf(sigma |0, 1);\n    \n    // Likelihood\n    if(run_estimation==1){\n        target += normal_lpdf(log_sales_price | alpha[neighbourhood] + beta * log_lot_area, sigma);\n\n    }\n}\ngenerated quantities {\n    // Uses fitted model to generate values of interest without re running the sampler\n    vector[N] log_lik; // Log likelihood\n    vector[N] y_hat; // Predictions using training data\n    {\n    for(n in 1:N){\n          log_lik[n] = normal_lpdf(log_sales_price | alpha[neighbourhood[n]] + beta * log_lot_area[n], sigma);\n          y_hat[n] = normal_rng(alpha[neighbourhood[n]] + beta * log_lot_area[n], sigma);      \n        }\n    }\n}\n\"\n\n# List contains all data inputs\nnpm_data_check = list(N = nrow(df),\n                      log_sales_price = as.vector(df$log_sales_price_z),\n                      log_lot_area = as.vector(df$log_lot_area_z),\n                      neighbourhood = as.vector(df$neighbourhood),\n                      N_neighbourhood = max(df$neighbourhood),\n                      alpha_sd = 1, \n                      beta_sd = 1, \n                      run_estimation = 0)\n\n# Draw samples from joint prior distribution\nfit_npm_check = stan(model_code = no_pooling_stan_code, data = npm_data_check, chains = 4, seed = 12345)\n\n# Extract samples into a pandas dataframe\nnpm_df_check = as.data.frame(fit_npm_check)\n\n\nFor the prior predictive checks, we recommend not cleaving too closely to the observed data and instead aiming for a prior data generating process that can produce plausible data sets, not necessarily ones that are indistinguishable from observed data. - Visualisation Bayesian Workflow - Gabry, Simpson, Vehtari, Betancourt, Gelman (2019)\n\nThe implied predictions of our priors are visualised below. I’ve arbitrarily chosen the 4th neighbourhood index (\\(\\alpha_{j=4}\\)) since the priors for the neighbourhoods are the same. Weakly informative priors should create bounds between possible values while allowing for some implausible relationships. Remembering that 95% of gaussian mass exists within two standard deviations of the mean is a useful guide for determining what is reasonable.\nLet’s see an example of setting uninformative priors and its implications of the data generating process. I’ve set the scale parameters for \\(\\alpha\\) and \\(\\beta\\) to be 10 which are quite diffuse. The implied predictions of the mean are much wider and well beyond the minimum and maximum values in the real data. This suggests that the model is giving too much probabilistic weight to highly implausible datasets.\n\n# Fit model with diffuse priors\nnpm_data_check_wide = list(N = nrow(df),\n                      log_sales_price = as.vector(df$log_sales_price_z),\n                      log_lot_area = as.vector(df$log_lot_area_z),\n                      neighbourhood = as.vector(df$neighbourhood),\n                      N_neighbourhood = max(df$neighbourhood),\n                      alpha_sd = 10, \n                      beta_sd = 10, \n                      run_estimation = 0)\n\nfit_npm_check_wide = stan(model_code = no_pooling_stan_code, data=npm_data_check_wide, chains = 4, seed = 12345)\nnpm_df_check_wide = as.data.frame(fit_npm_check_wide)\n\n# Create length of std x variables\nx &lt;- seq(from = -3, to = 3, length.out = 200)\n\n# Create empty dataframe and fill it with parameters\ndf_wide &lt;- as.data.frame(matrix(ncol=100, nrow=200))\nfor (i in 1:100) {\n  alpha &lt;- npm_df_check_wide$`alpha[4]`[i]\n  beta &lt;- npm_df_check_wide$beta[i]\n  df_wide[, i] &lt;- alpha + beta * x\n  \n}\n\n# Tidy up filled dataframe\ndf_wide &lt;- df_wide %&gt;% mutate(x = x) %&gt;% pivot_longer(starts_with(\"V\"))\n\n# Plot\nggplot(df_wide, aes(x = x, y = value)) +\n  geom_line(aes(group = name), size = 0.2) +\n  scale_x_continuous(breaks = seq(-3, 3, 1)) +\n  labs(title = 'Prior predictive checks -- Uninformative (flat) priors',\n             x = 'x (z-scores)',\n             y = 'Fitted y (z_scores)')\n\n\n\n\nOur original scale parameters of 1 produce more reasonable relationships. There are still some extreme regression lines implied by our data generating process, but they are bound to more realistic outcomes relative to the diffuse priors.\n\n# Create length of std x variables\nx &lt;- seq(from = -3, to = 3, length.out = 200)\n\n# Create empty dataframe and fill it with parameters\ndf_regularising &lt;- as.data.frame(matrix(ncol=100, nrow=200))\nfor (i in 1:100) {\n  alpha &lt;- npm_df_check$`alpha[4]`[i]\n  beta &lt;- npm_df_check$beta[i]\n  df_regularising[, i] &lt;- alpha + beta * x\n  \n}\n\n# Tidy up filled dataframe\ndf_regularising &lt;- df_regularising %&gt;% mutate(x = x) %&gt;% pivot_longer(starts_with(\"V\"))\n\n# Plot\nggplot(df_regularising, aes(x = x, y = value)) +\n  geom_line(aes(group = name), size = 0.2) +\n  scale_x_continuous(breaks = seq(-3, 3, 1)) +\n  labs(title = 'Prior predictive checks -- Weakly regularizing priors',\n             x = 'x (z-scores)',\n             y = 'Fitted y (z_scores)')\n\n\n\n\nPutting both sets of lines on the same scale emphasises the difference in simulated values. The blue lines from the previous graph cover a tighter space relative to the simulations from the uninformative priors.\n\nggplot(df_wide, aes(x = x, y = value)) +\n  geom_line(aes(group = name), size = 0.2) +\n  geom_line(data = df_regularising, aes(group = name), size = 0.2, colour = 'blue') +\n  scale_x_continuous(breaks = seq(-3, 3, 1)) +\n  labs(title = 'Prior predictive checks -- Uninformative (flat) priors',\n             x = 'x (z-scores)',\n             y = 'Fitted y (z_scores)') \n\n\n\n\n\n\n4) Fit model on fake data\nWe can use the simulations to see if our model can successfully estimate the parameters used to generate fake data (the implied \\(\\hat{y}\\)). Take a draw from the prior samples (e.g. the 50th simulation) and estimate the model on the data produced by these parameters. Let’s see if the model fitted on fake data can capture the “true” parameters (dotted red lines) of the data generating process. If the model cannot capture the known parameters which generated fake data, there is no certainty it will be estimating the correct parameters on real data.\n\n# Pick random simulation, let's say 50\nrandom_draw &lt;- 50\n\n# Extract the simulated (fake) data implied by the parameters in sample 50\ny_sim &lt;-  npm_df_check[random_draw, ] %&gt;% select(contains('y_hat')) %&gt;% t()\n\n# Extract the parameters corresponding to sample 50\ntrue_parameters = npm_df_check[random_draw,] %&gt;% select(contains(c('alpha','beta','sigma')))\n\n# List contains all data inputs\nnpm_data_check_ = list(N = nrow(df),\n                      log_sales_price = as.vector(y_sim), # target is now extracted fake data in sample 50\n                      log_lot_area = as.vector(df$log_lot_area_z),\n                      neighbourhood = as.vector(df$neighbourhood),\n                      N_neighbourhood = max(df$neighbourhood),\n                      alpha_sd = 1, \n                      beta_sd = 1, \n                      run_estimation = 1)\n\n# Fit the model on the fake data\nfit_npm_check_ = stan(model_code = no_pooling_stan_code, data=npm_data_check_, chains = 4, seed = 12345)\nnpm_df_check_ = as.data.frame(fit_npm_check_)\n\n# Extract parameters and tidy dataframe\nfake_fit = npm_df_check_ %&gt;% select(contains(c('alpha', 'beta', 'sigma')))\nparameter_df = fake_fit %&gt;% pivot_longer(everything()) %&gt;% rename(parameters = name)\nparameter_df$parameters &lt;- factor(parameter_df$parameters, levels = (parameter_df$parameters %&gt;% unique()))\n\n# Plot will give distributions of all parameters to see if it can capture the known parameters\nggplot(parameter_df, aes(value)) + \n  geom_density(colour = 'blue') + \n  facet_wrap(~parameters, scales = 'free') + \n  geom_vline(data = (true_parameters %&gt;% pivot_longer(everything()) %&gt;% rename(parameters = name)), aes(xintercept = value), colour = 'red') + \n  labs(title = 'Model Checking - red lines are \"true\" parameters',\n       x = '') + \n  theme(strip.background = element_blank()) \n\n\n\n\n\n\n5) Estimate model on real data\nSet run_estimation=1 and run the code to fit the model. Stan will sample the joint posterior distribution using the default Markov chain Monte Carlo (MCMC) algorithm, the No-U-Turn sampler (NUTs).\n\n# Dictionary with data inputs - set run_estimation=1\nnpm_data = list(N = nrow(df),\n                      log_sales_price = as.vector(df$log_sales_price_z),\n                      log_lot_area = as.vector(df$log_lot_area_z),\n                      neighbourhood = as.vector(df$neighbourhood),\n                      N_neighbourhood = max(df$neighbourhood),\n                      alpha_sd = 1, \n                      beta_sd = 1, \n                      run_estimation = 1)\n\n# Fit model by sampling from posterior distribution\nfit_npm = stan(model_code = no_pooling_stan_code, data = npm_data, chains = 4, seed = 12345)\n\n# Extract samples into dataframe\nfit_npm_df = as.data.frame(fit_npm)\n\n\n\n6) Check whether MCMC sampler and model fit\nStan won’t have trouble sampling from such a simple model, so I won’t go through chain diagnostics in detail. I’ve included number of effective samples and Rhat diagnostics for completeness. We can see the posterior distributions of all the parameters by looking at the traceplot as well.\n\nTraceplot\n\n# Inspect model fit\ncolor_scheme_set(\"mix-blue-red\")\n\nmcmc_combo(\n as.array(fit_npm),\n combo = c(\"dens_overlay\", \"trace\"),\n pars = c('alpha[1]', 'beta', 'sigma'),\n gg_theme = legend_none()) \n\n\n\n\n\n\nPosterior distributions\n\nstan_plot(fit_npm, \n          show_density = FALSE, \n          unconstrain = TRUE, \n          pars = c('alpha', 'beta', 'sigma')) + \n  labs(title = 'Posterior distributions of fitted parameters')\n\n\n\n\n\n\nneff / Rhat\n\nprint(fit_npm, pars = c('alpha', 'beta', 'sigma'), \n                         probs=c(0.025, 0.50, 0.975), \n                         digits_summary=3)\n\nInference for Stan model: 2be0e54fe1314f469f9b784aa4444aba.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n            mean se_mean    sd   2.5%    50%  97.5% n_eff  Rhat\nalpha[1]   1.005   0.002 0.148  0.708  1.007  1.291  5778 1.000\nalpha[2]   0.565   0.005 0.395 -0.218  0.567  1.348  7517 1.000\nalpha[3]  -0.099   0.002 0.165 -0.426 -0.099  0.221  5493 1.000\nalpha[4]  -0.687   0.001 0.083 -0.855 -0.686 -0.523  7361 1.000\nalpha[5]   0.001   0.001 0.122 -0.237  0.001  0.241  6665 0.999\nalpha[6]   0.330   0.000 0.050  0.233  0.330  0.426 10574 0.999\nalpha[7]   0.340   0.001 0.086  0.175  0.340  0.506  7469 0.999\nalpha[8]  -0.777   0.001 0.061 -0.897 -0.776 -0.656  7542 0.999\nalpha[9]   0.216   0.001 0.071  0.073  0.216  0.355  7027 1.000\nalpha[10] -1.331   0.001 0.098 -1.524 -1.330 -1.136  8231 0.999\nalpha[11] -0.406   0.002 0.151 -0.705 -0.406 -0.100  5802 0.999\nalpha[12] -0.320   0.001 0.085 -0.488 -0.321 -0.153  7145 1.000\nalpha[13] -0.440   0.000 0.041 -0.519 -0.442 -0.361  7392 1.000\nalpha[14]  1.369   0.001 0.096  1.177  1.369  1.559  7084 0.999\nalpha[15]  0.315   0.002 0.199 -0.083  0.315  0.704  7174 1.000\nalpha[16]  1.412   0.001 0.070  1.274  1.412  1.548  6393 0.999\nalpha[17]  0.100   0.001 0.071 -0.039  0.099  0.244  7724 0.999\nalpha[18] -0.684   0.001 0.057 -0.797 -0.683 -0.574  8749 1.000\nalpha[19] -0.596   0.001 0.069 -0.730 -0.596 -0.461  6336 0.999\nalpha[20]  0.121   0.001 0.079 -0.036  0.122  0.282  7308 1.000\nalpha[21]  0.869   0.001 0.067  0.738  0.869  1.002  6553 1.000\nalpha[22]  1.422   0.001 0.122  1.181  1.421  1.667  7990 1.000\nalpha[23] -0.357   0.001 0.121 -0.586 -0.357 -0.122  7743 0.999\nalpha[24]  0.502   0.001 0.099  0.311  0.501  0.692  7553 0.999\nalpha[25]  0.517   0.002 0.181  0.166  0.516  0.881  8580 0.999\nbeta       0.348   0.000 0.021  0.307  0.348  0.389  3651 1.000\nsigma      0.607   0.000 0.011  0.585  0.607  0.630  8275 1.000\n\nSamples were drawn using NUTS(diag_e) at Thu Nov 12 11:58:44 2020.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\n\n7) Posterior predictive check to evaluate model fit\nHow well did the model perform? We can perform posterior predictive checks to see if sampled distributions from the fitted model can approximate the density of SalesPrice. If the model performs well, it should be able to retrodict the density of the data used to train the model. The blue lines are the predictions drawn from the joint posterior distribution compared with the observed density of the target \\(y\\) variable.\n\n# Select 300 samples to plot against observed distribution\ncolor_scheme_set(scheme = \"blue\")\nyrep &lt;- extract(fit_npm)[[\"y_hat\"]]\nsamples &lt;- sample(nrow(yrep), 300)\nppc_dens_overlay(as.vector(df$log_sales_price_z), yrep[samples, ])\n\n\n\n\nReversing the data transformations gives back the posterior predictive checks on the natural scale (rescale \\(y\\) and exponentiate log(SalesPrice) to get back SalesPrice):\n\n# Take 300 samples of posterior predictive checks and revert back to natural scale\nppc &lt;- yrep[samples, ] %&gt;% \n  t() %&gt;% \n  apply(., MARGIN  = 2, FUN = function(x) exp((x * sd(df$log_sales_price)) + mean(df$log_sales_price))) %&gt;% \n  as.data.frame() %&gt;%\n  pivot_longer(everything())\n\n# Plot densities\nggplot(ppc, aes(value)) + \n  geom_density(aes(group = name), colour = \"lightblue\") + \n  geom_density(data = (df %&gt;% select(SalePrice) %&gt;% rename(value = SalePrice)), colour = 'black') +\n  theme(legend.position=\"none\", axis.text.y=element_blank()) +\n  labs(title = 'Posterior predictive checks - Black: observed SalePrice\\nLight Blue: Posterior Samples') +\n  ggsave('figures/9r_posterior_predictive_check_outcomescale.png', dpi = 300, height = 6, width = 9) \n\n\n\n\nNot bad for a simple model. There is definitely room for iteration and improvement."
  },
  {
    "objectID": "posts/bayesian_workflow_stats_rethinking/applying_a_bayesian_workflow_pt1_post.html#conclusion",
    "href": "posts/bayesian_workflow_stats_rethinking/applying_a_bayesian_workflow_pt1_post.html#conclusion",
    "title": "Applying a bayesian workflow: lessons from Statistical Rethinking (I)",
    "section": "Conclusion",
    "text": "Conclusion\nThe last thing we should do is compare the fits of multiple models and evaluate their performance using cross validation for model selection. The next post applies the full workflow using multilevel models and compares performance using techniques such as Leave One Out - Cross Validation (LOO-CV). Model performance can also be evaluated on out of sample test data as well since this is a predictive task (Kaggle computes the log RMSE of the out of sample dataset).\nThis is not an exhaustive review of all the diagnostics and visualisations that can be performed in a workflow. There are many ways of evaluating model fit and diagnostics that could validate or invalidate the model. Below are a list of resources which give more detailed examples on various bayesian models and workflows:\n\nStan case studies\nPyMC3 examples\nMichael Betancourt’s case study on a Principled Bayesian Workflow and all his other case studies\nBayesian Workflow and some links to the development of bayesian workflow over the past few years can be found here\nRobust Statistical Workflow with PyStan\nRobust Statistical Workflow with RStan\n\nNotebooks that reproduce the models/plots/etc:\nPython\nR\nReturn home"
  },
  {
    "objectID": "posts/bayesian_workflow_stats_rethinking/applying_a_bayesian_workflow_pt1_post.html#original-computing-environment",
    "href": "posts/bayesian_workflow_stats_rethinking/applying_a_bayesian_workflow_pt1_post.html#original-computing-environment",
    "title": "Applying a bayesian workflow: lessons from Statistical Rethinking (I)",
    "section": "Original Computing Environment",
    "text": "Original Computing Environment\n%load_ext watermark\n%watermark -n -v -u -iv -w -a Benjamin_Wee\n\nseaborn 0.11.0\npandas  1.1.3\narviz   0.10.0\npystan  2.19.0.0\nnumpy   1.19.1\nBenjamin_Wee \nlast updated: Thu Nov 19 2020 \n\nCPython 3.6.12\nIPython 5.8.0\nwatermark 2.0.2\nsessionInfo()\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Mojave 10.14.6\n\nMatrix products: default\nBLAS:   /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nRandom number generation:\n RNG:     Mersenne-Twister \n Normal:  Inversion \n Sample:  Rounding \n \nlocale:\n[1] en_AU.UTF-8/en_AU.UTF-8/en_AU.UTF-8/C/en_AU.UTF-8/en_AU.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] gridExtra_2.3        gdtools_0.2.2        svglite_1.2.3.2      bayesplot_1.7.2      rstan_2.21.2        \n [6] StanHeaders_2.21.0-6 forcats_0.5.0        stringr_1.4.0        dplyr_1.0.2          purrr_0.3.4         \n[11] readr_1.4.0          tidyr_1.1.2          tibble_3.0.4         ggplot2_3.3.2        tidyverse_1.3.0     \n\nloaded via a namespace (and not attached):\n [1] httr_1.4.2         jsonlite_1.7.1     splines_4.0.3      modelr_0.1.8       RcppParallel_5.0.2 assertthat_0.2.1  \n [7] stats4_4.0.3       cellranger_1.1.0   yaml_2.2.1         pillar_1.4.6       backports_1.2.0    lattice_0.20-41   \n[13] reticulate_1.18    glue_1.4.2         digest_0.6.27      rvest_0.3.6        colorspace_1.4-1   htmltools_0.5.0   \n[19] Matrix_1.2-18      plyr_1.8.6         pkgconfig_2.0.3    broom_0.7.2        haven_2.3.1        scales_1.1.1      \n[25] processx_3.4.4     mgcv_1.8-33        generics_0.1.0     farver_2.0.3       ellipsis_0.3.1     withr_2.3.0       \n[31] cli_2.1.0          magrittr_1.5       crayon_1.3.4       readxl_1.3.1       evaluate_0.14      ps_1.4.0          \n[37] fs_1.5.0           fansi_0.4.1        nlme_3.1-149       xml2_1.3.2         pkgbuild_1.1.0     tools_4.0.3       \n[43] loo_2.3.1          prettyunits_1.1.1  hms_0.5.3          lifecycle_0.2.0    matrixStats_0.57.0 V8_3.4.0          \n[49] munsell_0.5.0      reprex_0.3.0       callr_3.5.1        compiler_4.0.3     systemfonts_0.3.2  rlang_0.4.8       \n[55] grid_4.0.3         ggridges_0.5.2     rstudioapi_0.11    labeling_0.4.2     rmarkdown_2.5      gtable_0.3.0      \n[61] codetools_0.2-16   inline_0.3.16      DBI_1.1.0          curl_4.3           reshape2_1.4.4     R6_2.5.0          \n[67] lubridate_1.7.9    knitr_1.30         utf8_1.1.4         stringi_1.5.3      parallel_4.0.3     Rcpp_1.0.5        \n[73] vctrs_0.3.4        dbplyr_2.0.0       tidyselect_1.1.0   xfun_0.19"
  },
  {
    "objectID": "posts/masters_presentation_adelaide/index.html",
    "href": "posts/masters_presentation_adelaide/index.html",
    "title": "Master’s presentation to Adelaide data science group",
    "section": "",
    "text": "Master’s research presentation on stochastic volatility and simulation based calibration to the Adelaide data science group on November 2023.\n\nAbstract\nSimulation Based Calibration (SBC) (Talts, Betancourt, Simpson, Vehtari, & Gelman, 2020) is applied to analyse two commonly used, competing Markov chain Monte Carlo algorithms for estimating the posterior distribution of a stochastic volatility model. In particular, the bespoke ‘off-set mixture approximation’ algorithm proposed by Kim, Shephard, and Chib (1998) is explored together with a Hamiltonian Monte Carlo algorithm implemented through Stan (Stan Development Team, 2023). The SBC analysis involves a simulation study to assess whether each sampling algorithm has the capacity to produce valid inference for the correctly specified model, while also characterising statistical efficiency through the effective sample size. Results show that Stan’s No-U-Turn sampler, an implementation of Hamiltonian Monte Carlo, produces a well-calibrated posterior estimate while the celebrated off-set mixture approach is less efficient and poorly calibrated, though model parameterisation also plays a role."
  },
  {
    "objectID": "posts/power_sim/power_sim.html",
    "href": "posts/power_sim/power_sim.html",
    "title": "Power analysis by simulation",
    "section": "",
    "text": "Preamble\nThis blog post is taken from notes of a write-up I did at a previous job a few years ago when I was conducting and designing A/B tests for marketing teams. It is a condensed summary of my understanding of power analysis by simulation inspired by Nick Huntington-Klein’s excellent lecture notes on the same topic. Writing things up is a useful way of structuring my thoughts and ensuring I understand what I’m reading. The original write-up I did also had a Bayesian implementation (or interpretation) of power analysis but I’ve omitted it since I’m not 100% convinced it is the correct approach for pre-experimental design or whether other simulation frameworks are more appropriate. So I’ve stuck with the standard frequentist approach. Anyway, hope this is useful!\n\n\nIntroduction\nA common task then designing experiments is to determine whether an experiment is sufficiently “powered”. That is, conditional on our parameter constraints and available data, how likely will our experiment and models or tests reject the null hypothesis given that the null is false. This requires assumptions on effect sizes, the type of underlying data (continuous, proportions), equal or unequal sample sizes, equal or unequal variances, etc.\nI’ve been asked many times throughout my career to size up an experiment using standard frequentist tools. However, every time I return tot he exercise I get swamped with the variety of calculators and statistical tests that can be used - most only valid under very specific conditions and for relatively simple experiments. Typically in most commercial experiment designs, there are a lot of restrictions which may impact these calculations that cannot be adjusted directly.\nSo I turned so simulating my power analysis. Simulation is a great tool for flexible estimating the impact of different parameters and assumptions. This is done by fitting many models (or running tests) on synthetic data generated from a known data generating process (DGP). The DGP can be adjusted based on our assumptions and constraints. This is especially useful when greater complexity is built into the experimental design (multiple treatments, additional co-variates, generalising the impact of multiple experiments, etc).\n\n\nSetup\nI conducted an old experiment that measured the impact of a marketing campaign on product uptake for a treatment group compared to a representative control group. The specified parameters were:\n\nBase control uptake: \\(p_c = 0.35\\)\nMinimum detectable effect of treatment \\(p_t = 0.36\\)\nConfidence: (\\(1 - \\alpha) = 0.95\\)\nPower: (\\(1 - \\beta) = 0.8\\)\nType: One sided\nSample size: n\n\nSo assuming a base product uptake of \\(p_c = 35\\%\\), the minimum effect to be detected at 80% power is 1% (\\(p_t-p_c\\)) with a 5% false positive rate under the null hypothesis.\nThere are two approaches to this analysis:\n\nCalculate power for a known n (since this size can be restricted at the very start)\nCalculate n required for a given power (more typical question that is asked)\n\nFor my use case, I was given a restricted n and had to think of different ways of determining power for asymmetric control and treatment group sizes, so I will demonstrate 1).\nTypical workflow\n\nSet parameters\nSimulate synthetic data given these parameters\nFit model on data / run statistical rest\nExtract desired estimates (p values of confidence intervals, test statistics)\nRepeat 1)-4) multiple times\nDetermine what proportion tests were “statistically significant” (power estimate)\n\n\n\nBenchmarking\nBefore running a simulation, let’s benchmark our results against a typical power analysis. Based on our parameter settings we need ~28,312 samples in both treatment and control. The goal is to replicate these results by simulation.\n\nRPython\n\n\n\npower.prop.test(p1 = 0.35,\n                p2 = 0.36,\n                power = 0.8,\n                sig.level = 0.05,\n                alternative = 'one.sided')\n\n\n     Two-sample comparison of proportions power calculation \n\n              n = 28311.97\n             p1 = 0.35\n             p2 = 0.36\n      sig.level = 0.05\n          power = 0.8\n    alternative = one.sided\n\nNOTE: n is number in *each* group\n\n\n\n\n\nimport statsmodels.stats.api as sms\n\neffect_size = sms.proportion_effectsize(0.36, 0.35)\nsms.NormalIndPower().solve_power(effect_size, power=0.8, alpha=0.05, ratio=1, alternative = \"larger\")\n\n28311.706512937755\n\n\n\n\n\n\n\nSynthetic data simulation based on parameter settings\nFirst, set parameter values. These can be changed and re-run for other yse cases.\n\nRPython\n\n\n\n# Parameters\nn &lt;- 28312*2         # Total sample size\npc &lt;- 0.35           # Success probability in control group (inferred from domain knowledge)\npt &lt;- 0.36           # Success probability in treatment group (minimum 'practical' effect size)\nn_sim &lt;- 1e3         # Number of simulations to run\ntreatment_prop &lt;- 0.5 # Proportion in treatment\none_sided &lt;- TRUE     # True is one sided test\nside &lt;- \"right\"       # Defaults to right sided test if one sided test. Input 'left' for left sided test\n\n# Sample size of each group\ncontrol_prop = 1 - treatment_prop # Proportion in control\nnt &lt;- n * treatment_prop           # Treatment group size\nnc &lt;- n * control_prop             # Control group size\n\n\n\n\n# Parameters\nn = 28312*2         # Total sample size\npc = 0.35           # Success probability in control group (inferred from domain knowledge)\npt = 0.36           # Success probability in treatment group (minimum 'practical' effect size)\nn_sim = 1e3         # Number of simulations to run\ntreatment_prop = 0.5 # Proportion in treatment\none_sided = True     # True is one sided test\nside = \"right\"      # Right sided test\n\n# Sample size of each group\ncontrol_prop = 1 - treatment_prop # Proportion in control\nnt = int(n * treatment_prop)      # Treatment group size\nnc = int(n * control_prop)        # Control group size\n\n\n\n\nNext, simulate bernoulli outcomes for treatment and control group based on above parameter settings. \\(y\\) is the outcome variable (1 for product uptake, 0 for no uptake) and x is a binary indicator variable (0 for control, 1 for treatment).\n\nRPython\n\n\n\nset.seed(2025)\n\n# Control group bernoulli outcomes with probability pc\nyc &lt;- rbinom(n = nc, size = 1, prob = pc)\n\n# Treatment group bernoulli outcomes with probability pt\nyt &lt;- rbinom(n = nt, size = 1, prob = pt)\n\n# Dummy variable 1= treatment, 0 = control.\n# Coefficient is the relative change in log odds of success if in treatment group\nxc &lt;- rep(0, nc)\nxt &lt;- rep(1, nt)\n\n# Bring together in a data frame\ndf &lt;- data.frame(y = c(yc, yt), x = c(xc, xt))\n\nhead(df)\n\n  y x\n1 1 0\n2 0 0\n3 0 0\n4 0 0\n5 1 0\n6 0 0\n\n\n\n\n\nimport random\n\nimport numpy as np\nimport pandas as pd\n\nrandom.seed(2025)\n\n# Control group bernoulli outcomes with probability pc\nyc = np.random.binomial(n=1, p=pc, size=nc)\n\n# Treatment group bernoulli outcomes with probability pt\nyt = np.random.binomial(n=1, p=pt, size=nt)\n\n# Dummy variable 1= treatment, 0 = control.\n# Coefficient is the relative change in log odds of success if in treatment group\nxc = np.repeat(0, nc)\nxt = np.repeat(1, nt)\n\n# Bring together in a data frame\ndf = pd.DataFrame({\"y\":np.concatenate([yc,yt]),\n                   \"const\": np.repeat(1, (nc+nt)),\n                   \"x\":np.concatenate([xc,xt])})\n\n\n\n\n\n\nBinomial GLM (Logistic regression)\nNext, fit a logistic regression on the synthetic data and run a hypothesis test on the treatment/control dummy variable \\(x\\). This will be compared to a two sample proportions t test.\nBut why regression? Well, most statistical tests and estimation procedures (t-tests, ANOVA, etc) are all cases of general linear models that can be estimated with regression. This gives us the most flexibility when considering even more complicated experimental designs (e.g. adding other x variables for pre treatment adjustment of certain demographic characteristics, stratification across multiple cohorts, etc). It also gives a clear estimation strategy instead of fumbling through the documentation of different statistical tests.\n\\[\n\\begin{aligned}\ny_i \\sim Binomial&(n_i,p_i) \\\\\nlog \\left(\\frac{p_i}{1-p_i}\\right) &= \\beta_0 + \\bf X \\beta_1 \\\\\nH_0: \\beta_1 &= 0 \\\\\nH_1: \\beta_1 &&gt; 0\n\\end{aligned}\n\\]\nSo the hypothesis to test is if \\(\\beta_1\\) is larger than 0, significant at the 5% level. If we were to run this experiment multiple times under the same parameter settings, we should expect to correctly reject the null in favour of the alternative hypothesis 80% of the time (although this does not tell us anything about the uncertainty of the effect size).\nFitting the model below gives a significant result. The next step is to run this exercise multiple times to get a power estimate.\n\nRPython\n\n\n\n# Estimate/fit logistic regression\nmodel &lt;- glm(y~x, data = df, family = 'binomial')\n\n# Model results\nsummary(model)\n\n\nCall:\nglm(formula = y ~ x, family = \"binomial\", data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.61767    0.01246 -49.582  &lt; 2e-16 ***\nx            0.05068    0.01755   2.887  0.00389 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 73742  on 56623  degrees of freedom\nResidual deviance: 73734  on 56622  degrees of freedom\nAIC: 73738\n\nNumber of Fisher Scoring iterations: 4\n\n# Extract p value and divide by 2 for one sided test (Wald test statistic is asymptotically z distributed)\ncoef(summary(model))[2,4]/2\n\n[1] 0.001943298\n\n\n\n\n\nimport statsmodels.api as sm\n\n# Estimate/fit logistic regression\nmodel = sm.GLM(df[[\"y\"]], df[[\"const\", \"x\"]], family=sm.families.Binomial())\nfit = model.fit()\n\n# Model results\nfit.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n56624\n\n\nModel:\nGLM\nDf Residuals:\n56622\n\n\nModel Family:\nBinomial\nDf Model:\n1\n\n\nLink Function:\nLogit\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-36731.\n\n\nDate:\nThu, 24 Apr 2025\nDeviance:\n73462.\n\n\nTime:\n14:15:36\nPearson chi2:\n5.66e+04\n\n\nNo. Iterations:\n4\nPseudo R-squ. (CS):\n0.0002309\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.6413\n0.013\n-51.297\n0.000\n-0.666\n-0.617\n\n\nx\n0.0636\n0.018\n3.616\n0.000\n0.029\n0.098\n\n\n\n\n# Extract p value and divide by 2 for one sided test (Wald test statistic is asymptotically z distributed)\nfit.pvalues[1]/2\n\n0.0001497166036840973\n\n\n\n\n\n\n\nSimulate multiple times and calculate power\nThe below for loop will run the same step as above 1000 times.\n\nRPython\n\n\n\npower_sim &lt;- function(seed, \n                      ncontrol=nc, \n                      propcontrol=pc, \n                      ntreatment=nt, \n                      proptreatment=pt, \n                      onesided=one_sided, \n                      left_right_side=side\n                      ) {\n  # Binomial GLM (logistic regression)\n  set.seed(seed)\n\n  # Generate synthetic data \n  # Data generating process governed by parameters pc and pt\n  # Control group bernoulli outcomes with probability pc\n  yc &lt;- rbinom(n = ncontrol, size = 1, prob = propcontrol)\n\n  # Treatment group bernoulli outcomes with probability pt\n  yt &lt;- rbinom(n = ntreatment, size = 1, prob = proptreatment)\n\n  # Dummy variable treatment = 1, control = 0\n  # Coefficient is the relative change in log odds of success if in treatment group\n  xc &lt;- rep(0, ncontrol)\n  xt &lt;- rep(1, ntreatment)\n\n  # Bring together in a dataframe\n  df &lt;- data.frame(y = c(yc, yt), x = c(xc, xt))\n\n  # Fit model\n  model &lt;- glm(y~x, data = df, family = 'binomial')\n\n  results &lt;- data.frame(\n    test_result = NA,\n    pvalues = NA\n  )\n\n  if(onesided == FALSE) {\n    # Extract p values, returns TRUE if less than 0.05, FALSE otherwise\n    results[\"pvalues\"] &lt;- coef(summary(model))[2,4]\n    results[\"test_result\"] &lt;- results[1,\"pvalues\"] &lt; 0.05\n  } else if (onesided == TRUE) {\n    # One sided test, halve the p-value\n    results[\"pvalues\"] &lt;- coef(summary(model))[2,4]/2\n\n    if(left_right_side == 'right') {\n      # Ensure test statistic is greater than the null hypothesis for right sided test\n       results[\"test_result\"] &lt;- results[1,\"pvalues\"] &lt; 0.05 & coef(summary(model))[2,1] &gt; 0\n    }\n    else if(left_right_side == 'left') {\n      # Test stat less than null for left sided test\n       results[\"test_result\"] &lt;- results[1,\"pvalues\"] &lt; 0.05 & coef(summary(model))[2,1] &lt; 0\n    } else {\n      stop('Error: one_sided must be TRUE or FALSE')\n    }\n  }\n  return(results)\n}\n\n# Random seed grid\nset.seed(456)\nrandom_grid &lt;- sample(1e6, n_sim)\n\n# Set up parallelism \navailable_cores = parallel::detectCores()-1\nfuture::plan(future::multisession, workers = available_cores)\n\nsim_list &lt;- future.apply::future_Map(function(x) power_sim(seed = x), x = random_grid, future.seed=TRUE)\nsim_df &lt;- do.call(rbind, sim_list)\n\nFinally, calculating the power:\n\nmean(sim_df[[\"test_result\"]])\n\n[1] 0.791\n\n\n\n\n\nimport multiprocess as mp\n\ndef power_sim(seed, ncontrol=nc, propcontrol=pc, ntreatment=nt, proptreatment=pt, onesided=one_sided, \nleft_right_side=side):\n    np.random.seed(seed)\n    # Generate synthetic data \n    # Data generating process governed by parameters pc and pt\n    # Control group bernoulli outcomes with probability pc\n    yc = np.random.binomial(n=1, p=propcontrol, size=ncontrol)\n    \n    # Treatment group bernoulli outcomes with probability pt\n    yt = np.random.binomial(n=1, p=proptreatment, size=ntreatment)\n    \n    # Dummy variable treatment = 1, control = 0\n    # Coefficient is the relative change in log odds of success if in treatment group\n    xc = np.repeat(0, ncontrol)\n    xt = np.repeat(1, ntreatment)\n    \n    # Bring together in a dataframe\n    df = pd.DataFrame({\"y\":np.concatenate([yc,yt]),\n                     \"const\": np.repeat(1, (ncontrol+ntreatment)),\n                     \"x\":np.concatenate([xc,xt])})\n    \n    # Fit model\n    model = sm.GLM(df[[\"y\"]], df[[\"const\", \"x\"]], family=sm.families.Binomial())\n    fit = model.fit()\n    \n    if not onesided:\n        # Extract p values, returns TRUE if less than 0.05, FALSE otherwise\n        pvalue = fit.pvalues[1]\n        test_result = pvalue &lt; 0.05\n    elif onesided:\n        # One sided test, halve the p-value\n        pvalue = fit.pvalues[1]/2\n        \n        if left_right_side == 'right':\n            # Ensure test statistic is greater than the null hypothesis for right sided test\n            test_result = pvalue &lt; 0.05 and fit.params[1] &gt; 0\n            \n        elif left_right_side == 'left':\n            # Test stat less than null for left sided test\n            test_result = pvalue &lt; 0.05 and fit.params[1] &lt; 0\n            \n    else:\n      Exception('Error: one_sided must be TRUE or FALSE')\n    \n    return tuple([pvalue, test_result])\n\n# Random seed grid\nrandom.seed(2025)\nrandom_grid = list(np.random.randint(0,int(1e6), int(n_sim)))\n\n# Set up parallelism \nnprocs = mp.Pool()._processes-1\npool = mp.Pool(processes=nprocs)\nresult = pool.map_async(power_sim, random_grid)\nresult_ls = result.get()\nresult_df = pd.DataFrame(result_ls)\nresult_df.columns = [\"pvalues\", \"test_result\"]\n\nFinally, calculating the power:\n\nnp.mean(result_df[[\"test_result\"]])\n\n0.784\n\n\n\n\n\nGreat! The null was rejected for around 80% of simulations which is in line with the original power analysis. Increasing the simulation number should see this get closer to 80%.\nThe original use case of this experiment was to consider different treatment and control group sizes. The original power test analysis assumes equal, independent sample sizes in treatment and control. Instead of looking for an appropriate statistical test, I can just reset the parameters above to have 70% treatment and 30% control proportions. This enables the most flexibility when it comes to pre-analysis design since I can simulate data with other properties for the use case at hand."
  },
  {
    "objectID": "posts/learning_matplotlib/2020-04-06_Learning_matplotlib.html",
    "href": "posts/learning_matplotlib/2020-04-06_Learning_matplotlib.html",
    "title": "Learning Matplotlib - Lessons from a ggplot user",
    "section": "",
    "text": "I love R. I wouldn’t be where I am in my career without the tools of the Tidyverse and kindness of the Rstats and Rladies communities. It provided me a platform to entering data science, exposing me to the world of software engineering and development along the way. Choosing to use R over Stata for my Economics Honours thesis was probably the best choice I made (hides behind shield).\nI picked up new programming languages over the past two years working as a data scientist. The choice of programming languages at work are determined by the design of the tech stack and the analytics teams. We mostly use PySpark at work for exploratory data analysis (EDA) and “feature engineering”. So far, I’ve managed. I picked up standard python libraries like Pandas and NumPy, machine learning libaries (MLlib) in Pyspark, and wrangled many tables in SQL.\nBut the one thing I couldn’t put myself through is learning how to plot in Python. I couldn’t get over the ease and beauty of building visualisations using ggplot2. I would do all my work in SQL and pyspark, but I would always finish off my EDA using ggplot."
  },
  {
    "objectID": "posts/learning_matplotlib/2020-04-06_Learning_matplotlib.html#processing-the-data",
    "href": "posts/learning_matplotlib/2020-04-06_Learning_matplotlib.html#processing-the-data",
    "title": "Learning Matplotlib - Lessons from a ggplot user",
    "section": "Processing the data",
    "text": "Processing the data\nMost of the data preprocessing has been completed in the R script. However, there is still a bit of work requierd to get the necessary datasets and objects prepared for plotting. The exact data and scripts I used here\nFirst, load in the data, libraries and clean up some of the country labels.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as tick\n\n# Setting some Matplotlib parameters\nmpl.rcParams['figure.dpi']= 300\n# plt.style.use('seaborn-talk')\n\n# Replace the some of the country names with shorter labels\ndata = pd.read_csv('cov_case_curve2.csv')\\\n         .replace({'cname' : \n                   {'United States' : 'USA',\n                   'Iran, Islamic Republic of' : 'Iran',\n                   'Korea, Republic of' : 'South Korea', \n                   'United Kingdom' : 'UK'}})\\\n        .drop('Unnamed: 0', axis=1)\n\n# Show first 5 rows of the main dataset\ndata.sort_values(\"date\", ascending = False)\\\n    .head()\\\n    .style\n\n\n\n\n\n\n \ndate\ncname\niso3\ncases\ndeaths\ncu_cases\ncu_deaths\ndays_elapsed\nend_label\n\n\n\n\n2205\n2020-04-06\nViet Nam\nVNM\n1\n0\n241\n0\n14\nViet Nam\n\n\n2146\n2020-04-06\nJordan\nJOR\n22\n0\n345\n5\n13\nJordan\n\n\n2118\n2020-04-06\nDenmark\nDNK\n292\n18\n4369\n179\n27\nDenmark\n\n\n2119\n2020-04-06\nDominican Republic\nDOM\n167\n5\n1745\n82\n15\nDominican Republic\n\n\n2120\n2020-04-06\nEcuador\nECU\n181\n8\n3646\n180\n19\nEcuador\n\n\n\n\n\nNext, we subset our data so it contains the top 50 cumulative cases as of April 6th (df_50) and pivot them into a ‘wide’ format. The index of the dataframes will be the days_elapsed variable (x-axis) and the columns will contain cumulative cases for each country (y-axis, presented on log10 scale).\nFor ggplot and seaborn, these country columns would be their own variable to determine the facets of the plot. This makes creating the grid of a facet plot much simpler (one line of code).\n\n## Countries - top 50 cumulative cases as of 2020-03-27\n## We will use this to create a dataset with only the top 50 countries\n## that we will highlight in all the subplots\ntop_50 = data.loc[data.groupby([\"cname\"])[\"cu_cases\"].idxmax()]\\\n             .sort_values('cu_cases', ascending = False)\\\n             .head(50)\\\n             .loc[:, ['iso3', 'cname', 'cu_cases']]\n\n## Filter countries in top 50\n_df = data.loc[data['iso3'].isin(top_50['iso3'])]\\\n\n## Restructure data into wide format\n## Top 50\ndf_50 = _df.pivot(index = 'days_elapsed', \n                  values = 'cu_cases',\n                  columns = 'cname')\\\n          .reset_index(inplace=False)\n\n# Days elapsed as index (x axis)\ndf_50.index = df_50['days_elapsed']\n\n# Drop unwanted columns\n#df = df.drop('days_elapsed', axis = 1)\ndf_50 = df_50.drop('days_elapsed', axis = 1)\n\n# Display one of the dataframes\ndf_50.head().style\n\n\n\n\n\n\ncname\nAlgeria\nArgentina\nAustralia\nAustria\nBelgium\nBrazil\nCanada\nChile\nChina\nColombia\nCzech Republic\nDenmark\nDominican Republic\nEcuador\nFinland\nFrance\nGermany\nIceland\nIndia\nIndonesia\nIran\nIreland\nIsrael\nItaly\nJapan\nLuxembourg\nMalaysia\nMexico\nNetherlands\nNorway\nPakistan\nPanama\nPeru\nPhilippines\nPoland\nPortugal\nQatar\nRomania\nRussian Federation\nSaudi Arabia\nSerbia\nSouth Africa\nSouth Korea\nSpain\nSweden\nSwitzerland\nThailand\nTurkey\nUSA\nUnited Arab Emirates\n\n\ndays_elapsed\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n0\n102.000000\n128.000000\n100.000000\n102.000000\n109.000000\n121.000000\n103.000000\n156.000000\n216.000000\n102.000000\n116.000000\n113.000000\n112.000000\n111.000000\n155.000000\n100.000000\n111.000000\n117.000000\n125.000000\n117.000000\n139.000000\n129.000000\n116.000000\n132.000000\n105.000000\n140.000000\n117.000000\n118.000000\n128.000000\n113.000000\n187.000000\n109.000000\n117.000000\n111.000000\n104.000000\n112.000000\n262.000000\n113.000000\n114.000000\n118.000000\n126.000000\n116.000000\n155.000000\n114.000000\n137.000000\n209.000000\n114.000000\n191.000000\n103.000000\n113.000000\n\n\n1\n189.000000\n158.000000\n112.000000\n131.000000\n169.000000\n200.000000\n138.000000\n201.000000\n235.000000\n128.000000\n150.000000\n264.000000\n202.000000\n168.000000\nnan\n130.000000\n129.000000\n117.000000\n137.000000\n134.000000\n245.000000\n169.000000\n178.000000\n229.000000\n132.000000\n210.000000\n129.000000\n164.000000\n188.000000\n147.000000\n187.000000\n137.000000\n145.000000\n140.000000\n125.000000\n169.000000\nnan\n139.000000\n147.000000\n133.000000\n135.000000\n150.000000\n345.000000\n151.000000\n161.000000\n264.000000\n177.000000\n359.000000\n125.000000\n113.000000\n\n\n2\n231.000000\n225.000000\n126.000000\n182.000000\n200.000000\n234.000000\n176.000000\n238.000000\n386.000000\n158.000000\n214.000000\n516.000000\n245.000000\n199.000000\n210.000000\n178.000000\n157.000000\n138.000000\n165.000000\n172.000000\n388.000000\n223.000000\n250.000000\n322.000000\n144.000000\n345.000000\n149.000000\n203.000000\n265.000000\n169.000000\n302.000000\n200.000000\n234.000000\n142.000000\n177.000000\n245.000000\n320.000000\n184.000000\n199.000000\n133.000000\n149.000000\n205.000000\n601.000000\n200.000000\n203.000000\n332.000000\n177.000000\n670.000000\n159.000000\n140.000000\n\n\n3\n264.000000\n266.000000\n156.000000\n246.000000\n239.000000\n291.000000\n244.000000\n342.000000\n526.000000\n210.000000\n298.000000\n676.000000\n312.000000\n426.000000\n267.000000\n212.000000\n196.000000\n178.000000\n191.000000\n172.000000\n593.000000\n292.000000\n260.000000\n400.000000\n144.000000\n484.000000\n158.000000\n251.000000\n321.000000\n192.000000\n478.000000\n245.000000\n263.000000\n187.000000\n238.000000\n331.000000\n337.000000\n217.000000\n253.000000\n171.000000\n188.000000\n240.000000\n762.000000\n261.000000\n248.000000\n374.000000\n177.000000\n947.000000\n233.000000\n140.000000\n\n\n4\n305.000000\n301.000000\n197.000000\n361.000000\n267.000000\n428.000000\n304.000000\n434.000000\n623.000000\n235.000000\n344.000000\n804.000000\n392.000000\n532.000000\n272.000000\n285.000000\n262.000000\n199.000000\n231.000000\n227.000000\n978.000000\n366.000000\n427.000000\n650.000000\n164.000000\n670.000000\n197.000000\n316.000000\n382.000000\n277.000000\n495.000000\n313.000000\n318.000000\n202.000000\n287.000000\n448.000000\n401.000000\n260.000000\n306.000000\n238.000000\n222.000000\n274.000000\n892.000000\n374.000000\n326.000000\n490.000000\n212.000000\n1236.000000\n338.000000\n153.000000"
  },
  {
    "objectID": "posts/learning_matplotlib/2020-04-06_Learning_matplotlib.html#plotting-single-country",
    "href": "posts/learning_matplotlib/2020-04-06_Learning_matplotlib.html#plotting-single-country",
    "title": "Learning Matplotlib - Lessons from a ggplot user",
    "section": "Plotting single country",
    "text": "Plotting single country\nI started by plotting a single country before adding the other components of the final chart. Starting simple helped me get the basics right first. The below chart the cumulative growth of cases in China by days since the 100th case.\nYou can plot this using matplotlib or the plot function of the pandas dataframe. All the customisation uses matplotlib functions. The difference between using matplotlib and pandas.plot is minor in a simple plot. However, it is much easier to start off with pandas.plot then add customisation via matplotlib when it comes to more complicated visualisations (thanks to Chris Moffitt’s blog post on effective plotting in Python for this advice).\n\n## Plot China\nfig, ax = plt.subplots(figsize = (8, 13))\n\n# Matplotlib\nplt.plot(df_50.index, df_50['China'], color = 'grey')\n\n# Pandas\n# df_50['China'].plot(color='grey')\n\n# Plot customisation\nplt.title('Cumulative Growth rate of China')\nplt.xlabel('Days since 100th confirmed case')\nplt.ylabel('Cumulative number of cases (log10 scale)')\nax.set_xticks(np.arange(0, 80, 20), minor=False)\nax.set_yscale('log', base=10)\nax.yaxis.set_major_formatter(tick.ScalarFormatter())\nax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\nplt.plot();"
  },
  {
    "objectID": "posts/learning_matplotlib/2020-04-06_Learning_matplotlib.html#plotting-all-countries",
    "href": "posts/learning_matplotlib/2020-04-06_Learning_matplotlib.html#plotting-all-countries",
    "title": "Learning Matplotlib - Lessons from a ggplot user",
    "section": "Plotting all countries",
    "text": "Plotting all countries\nNext is to display the values of all coutries in a single figure. Matplotlib requires the code to explicitly identify which columns of the dataframe it is plotting. To do this I created a list of all countries which I loop over to plot each individual line. This will also be used to ‘highlight’ the relevant countries in each subplot later on.\nThe plot function in pandas is more forgiving. It will plot all columns for you automatically. No for loop needed.\n\n# Create list of countries to loop over\ncountries = top_50['cname'].drop_duplicates().tolist()\n\n# Print first 5 countries\ncountries[0:5]\n\n['USA', 'Spain', 'Italy', 'Germany', 'China']\n\n\n\nPandas\n\nfig, axes= plt.subplots(figsize=(5,8))\n\n# Pandas\ndf_50.plot(color='grey', alpha = 0.6, linewidth=0.5, legend = False, ax = axes)\n\n# Matplotlib customisation\nplt.title('Cumulative reported cases for all countries')\nplt.xlabel('Days since 100th confirmed case')\nplt.ylabel('Cumulative number of cases (log10 scale)')\naxes.set_xticks(np.arange(0, 80, 20), minor=False)\naxes.set_yscale('log', base=10)\naxes.grid(alpha=0.2)\naxes.yaxis.set_major_formatter(tick.ScalarFormatter())\naxes.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n\nplt.plot();\n\n\n\n\n\n\nMatplotlib\n\nfig, axes = plt.subplots(figsize = (5, 8))\n\n# Matplotlib\nfor idx, count in enumerate(countries):\n    plt.plot(df_50.index, df_50[str(count)], color = 'grey', alpha = 0.6, linewidth=0.5)\n\n# Matplotlib Customisation\nplt.title('Cumulative reported cases for all countries')\nplt.xlabel('Days since 100th confirmed case')\nplt.ylabel('Cumulative number of cases (log10 scale)')\naxes.set_xticks(np.arange(0, 80, 20), minor=False)\naxes.set_yscale('log', base=10)\naxes.yaxis.set_major_formatter(tick.ScalarFormatter())\naxes.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n\nplt.plot();"
  },
  {
    "objectID": "posts/learning_matplotlib/2020-04-06_Learning_matplotlib.html#plotting-all-countries-in-50-subplots",
    "href": "posts/learning_matplotlib/2020-04-06_Learning_matplotlib.html#plotting-all-countries-in-50-subplots",
    "title": "Learning Matplotlib - Lessons from a ggplot user",
    "section": "Plotting all countries in 50 subplots",
    "text": "Plotting all countries in 50 subplots\nThis is where things get complicated. We need to loop over the countries list for both matplotlib and pandas to create subplots for each of the top 50 countries. This means that matplotlib has another for loop to compute all the cumulative cases as well as doing it across subplots. I spent quite a few hours trying not to do this (I would’ve got this post out a lot earlier).\nBut it turns out the best way to do this is just plot the 50 countries using pandas.plot and use the for loop to plot across all subplots. I’ve kept my attempted matplotlib version in the tab below – please get in touch if you know a better way of doing this.\n\nPandas\n\nfig, axes = plt.subplots(10, 5, figsize = (16, 30), sharex = True, sharey = True)\n\nfor idx, count in enumerate(countries):\n    # Get grey lines for all subplots\n    df_50.plot(ax = axes[idx//5][idx%5],\n                legend = False, \n                color='grey',  \n                alpha = 0.6, \n                linewidth=0.5)\n\n    axes[idx//5][idx%5].set_title(str(count), size = 9)\n    axes[idx//5][idx%5].set_xlabel('')\n    axes[idx//5][idx%5].set_ylabel('')\n    axes[idx//5][idx%5].set_yscale('log', base=10)\n    axes[idx//5][idx%5].yaxis.set_major_formatter(tick.ScalarFormatter())\n    axes[idx//5][idx%5].get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n    axes[idx//5][idx%5].grid(alpha=0.1)\n    axes[idx//5][idx%5].set_xticks(np.arange(0, 80, 20), minor=False)\n    \nfig.suptitle('Cumulative Number of Reported Cases of COVID-19: Top 50 Countries', fontsize=20,\n            x=0.12, y=.91, horizontalalignment='left', verticalalignment='top')\nfig.text(0.12, 0.895, 'Date of Saturday, April 4, 2020', fontsize=16, ha='left', va='top')\nfig.text(0.04, 0.5, 'Cumulative number of cases (log10 scale)', va='center', rotation='vertical', size = 16) \nfig.text(0.5, 0.097, 'Days since 100th confirmed case', ha='center', size = 16)\nplt.figure();\n\n\n\nMatplotlib\n\nfig, axes = plt.subplots(10, 5, figsize = (16, 30), sharex = True, sharey = True)\n\nfor idx, count in enumerate(countries):\n    for country in enumerate(countries):\n        axes[idx//5][idx%5].plot(df_50.index, df_50[str(country[1])],\n                                 color='grey',  \n                                 alpha = 0.6, \n                                 linewidth=0.5)\n   \n        axes[idx//5][idx%5].title.set_text(str(count))\n        axes[idx//5][idx%5].set_title(str(count), size = 9)\n        axes[idx//5][idx%5].set_xlabel('')\n        axes[idx//5][idx%5].set_ylabel('')\n        axes[idx//5][idx%5].set_yscale('log', base=10)\n        axes[idx//5][idx%5].yaxis.set_major_formatter(tick.ScalarFormatter())\n        axes[idx//5][idx%5].get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n        axes[idx//5][idx%5].grid(alpha=0.2)\n        axes[idx//5][idx%5].set_xticks(np.arange(0, 80, 20), minor=False)\n\nfig.suptitle('Cumulative Number of Reported Cases of COVID-19: Top 50 Countries', fontsize=20,\n            x=0.12, y=.91, horizontalalignment='left', verticalalignment='top')\nfig.text(0.12, 0.895, 'Date of Saturday, April 4, 2020', fontsize=16, ha='left', va='top')\nfig.text(0.04, 0.5, 'Cumulative number of cases (log10 scale)', va='center', rotation='vertical', size = 16) \nfig.text(0.5, 0.097, 'Days since 100th confirmed case', ha='center', size = 16)\n        \nplt.plot();"
  },
  {
    "objectID": "posts/learning_matplotlib/2020-04-06_Learning_matplotlib.html#highlighting-countries-and-adding-points",
    "href": "posts/learning_matplotlib/2020-04-06_Learning_matplotlib.html#highlighting-countries-and-adding-points",
    "title": "Learning Matplotlib - Lessons from a ggplot user",
    "section": "Highlighting countries and adding points",
    "text": "Highlighting countries and adding points\nFinally all that’s left to do is to highlight the line corresponding to the country in the subplot and add a point for the end of each line. To do plot the points at the end of each red line, I create and loop over a dataframe which contains the top 50 countries with the corresponding latest day elapsed and cumulative case. This is completed using just pandas.plot and customised with matplotlib functions.\n\n# Subset dataframe with top 50 countries and the latest cumulative case value\nmarkers = data.loc[data.groupby([\"cname\"])[\"cu_cases\"].idxmax()]\\\n             .sort_values('cu_cases', ascending = False)\\\n             .head(50)\\\n             .loc[:, ['days_elapsed', 'cname', 'cu_cases']]\\\n             .reset_index(drop = True)\n\nmarkers.head().style\n\n\n\n\n\n\n \ndays_elapsed\ncname\ncu_cases\n\n\n\n\n0\n34\nUSA\n337635\n\n\n1\n34\nSpain\n130759\n\n\n2\n42\nItaly\n128948\n\n\n3\n36\nGermany\n95391\n\n\n4\n78\nChina\n82642\n\n\n\n\n\n\nfig, axes = plt.subplots(10, 5, figsize = (16, 30), sharex = True, sharey = True)\n\nfor idx, count in enumerate(countries):\n    # Get grey lines for all subplots\n    df_50.plot(ax = axes[idx//5][idx%5],\n                legend = False, \n                color='grey',  \n                alpha = 0.6, \n                linewidth=0.5)\n   \n    # Highlight relevant countries for each subplot\n    df_50[str(count)].plot(ax = axes[idx//5][idx%5],\n                            legend = False, \n                            color='red',\n                            linewidth=0.9)\n    \n    # Add markers at the end of each line\n    markers.query('cname == \"{}\"'.format(count))\\\n           .plot.scatter(ax = axes[idx//5][idx%5],\n                         x='days_elapsed', \n                         y='cu_cases', \n                         color = 'red')\n    \n    axes[idx//5][idx%5].set_title(str(count), size = 9)\n    axes[idx//5][idx%5].set_xlabel('')\n    axes[idx//5][idx%5].set_ylabel('')\n    axes[idx//5][idx%5].set_yscale('log', base=10)\n    axes[idx//5][idx%5].yaxis.set_major_formatter(tick.ScalarFormatter())\n    axes[idx//5][idx%5].get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n    axes[idx//5][idx%5].grid(alpha=0.1)\n    axes[idx//5][idx%5].set_xticks(np.arange(0, 80, 20), minor=False)\n    \nfig.suptitle('Cumulative Number of Reported Cases of COVID-19: Top 50 Countries', fontsize=20,\n            x=0.12, y=.91, horizontalalignment='left', verticalalignment='top')\nfig.text(0.12, 0.895, 'Date of Saturday, April 4, 2020', fontsize=16, ha='left', va='top')\nfig.text(0.04, 0.5, 'Cumulative number of cases (log10 scale)', va='center', rotation='vertical', size = 16) \nfig.text(0.5, 0.097, 'Days since 100th confirmed case', ha='center', size = 16)\n\nplt.figure();"
  },
  {
    "objectID": "posts/learning_matplotlib/2020-04-06_Learning_matplotlib.html#resources",
    "href": "posts/learning_matplotlib/2020-04-06_Learning_matplotlib.html#resources",
    "title": "Learning Matplotlib - Lessons from a ggplot user",
    "section": "Resources",
    "text": "Resources\nResources I found useful as an R user learning Python:\nEffectively using Matplotlib - great for outlining a set of principles and steps for plotting\nPandas comparison with R - Method chaining preserves the main functionality of the tidyverse %&gt;% pipe, just need to look up the corresponding functions\nPandas plotting - useful summary of pandas plotting tools"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Ben and I’m a data scientist in Melbourne looking to pivot into software (ML/data) engineering. I am interested in (too) many things. I love learning about Bayesian statistics, econometrics and data storytelling through modelling and visualisations. I also love building things with code and working with others to solve problems.\nI have spent the past 5 years working as a data scientist in industry. More recently I have found myself enjoying collaborating closely with data and machine learning engineers - learning about how to build and support production systems (machine learning models as well as data pipelines). I hope to continue doing this as I look to shift my career into the engineering space.\nI am currently studying a Masters of Applied Econometrics at Monash University, beginning in 2022 and studying part-time. Previously I completed an Honours degree in Economics from the University of Melbourne. I was also a research and teaching assistant (tutor) in the Economics department, teaching undergraduate classes in statistics and econometrics."
  }
]
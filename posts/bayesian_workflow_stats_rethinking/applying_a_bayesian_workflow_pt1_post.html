<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Benjamin Wee">
<meta name="dcterms.date" content="2020-11-21">

<title>Benjamin Wee - Applying a bayesian workflow: lessons from Statistical Rethinking (I)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Benjamin Wee</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/bennywee"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/BenwWee"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/benjamin-wee3"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Applying a bayesian workflow: lessons from Statistical Rethinking (I)</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Benjamin Wee </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 21, 2020</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#thinking-about-workflow" id="toc-thinking-about-workflow" class="nav-link active" data-scroll-target="#thinking-about-workflow">Thinking about workflow</a>
  <ul>
  <li><a href="#steps-in-proposed-workflow" id="toc-steps-in-proposed-workflow" class="nav-link" data-scroll-target="#steps-in-proposed-workflow"><strong>Steps in proposed workflow</strong></a></li>
  </ul></li>
  <li><a href="#predicting-housing-prices" id="toc-predicting-housing-prices" class="nav-link" data-scroll-target="#predicting-housing-prices">Predicting housing prices</a>
  <ul>
  <li><a href="#pick-your-language" id="toc-pick-your-language" class="nav-link" data-scroll-target="#pick-your-language">Pick your language</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#original-computing-environment" id="toc-original-computing-environment" class="nav-link" data-scroll-target="#original-computing-environment">Original Computing Environment</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>I spent the last few years studying Bayesian statistics in my spare time. Most recently, I completed Richard McElreath’s <a href="https://xcelab.net/rm/statistical-rethinking/">Statistical Rethinking</a> - including his 2017 lecture series and problem sets. It is rightfully one of the most popular entry level texts in bayesian statistics. I could not recommend it more highly.</p>
<p>While I’ve gained a lot from doing problem sets and discussing course material with other people, nothing beats testing your knowledge and challenging imposter syndrome by attempting a modelling problem on your own.</p>
<p>So I decided to apply what I’ve learned so far on the Kaggle dataset: <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview">House Prices: Advanced Regression Techniques</a>. The goal is to practise what I’ve learned in my self study journey and hopefully demonstrate some of the best practices advocated by those in the applied statistics community. This writeup was completed in R and Python (you’ll get to choose below) and Stan* for modelling.</p>
<p>*<a href="https://mc-stan.org/">Stan</a> is a probabilistic programming language that can flexibly estimate a wide range of probabilistic and bayesian models. It uses state of the art algorithms such as Hamiltonian Monte Carlo (HMC) which allows efficient sampling of models in high dimensions without the need for conjugate priors. Stan is used across academia and industry and notably in facebook’s open source forecasting tool, <a href="https://facebook.github.io/prophet/">prophet</a>.</p>
<section id="thinking-about-workflow" class="level2">
<h2 class="anchored" data-anchor-id="thinking-about-workflow">Thinking about workflow</h2>
<p>A recurring theme in applied statistics is the importance of workflow. This topic wasn’t really covered explicitly in my econometrics/stats classes which put more emphasis on tools and derivations for predictive or causal inference. At best, components of workflow were explored in some research seminars.</p>
<p>A good workflow supports quality model building. It forces us to think critically about decision making in data analysis which helps us evaluate our assumptions and identify errors. That isn’t to say there is a gold standard of how all data analysis should be conducted. Rather, following a robust methodology guides modelling decisions and helps diagnose problems. This becomes more important when adding complexity into models where it is harder to pinpoint where problems lie.</p>
<p>Developments around workflow are a current topic of research. The most recent paper came out on November 2nd titled <a href="https://arxiv.org/abs/2011.01808">Bayesian Workflow</a> which has many contributions from prominent members in the statistics community. This writeup was prepared before I had a chance to read the paper, but I hope it covers some of the principles and recommendations. And if not, like statistical models, the way I do data analysis will iterate and improve.</p>
<blockquote class="blockquote">
<p>“Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion” - <a href="https://arxiv.org/abs/1709.01449">Visualisation Bayesian Workflow - Gabry, Simpson, Vehtari, Betancourt, Gelman (2019)</a></p>
</blockquote>
<p>The proposed workflow I adopted was originally inspired from <a href="https://khakieconomics.github.io/2016/08/29/What-is-a-modern-statistical-workflow.html">blog posts</a> by Jim Savage a few years ago and more recently, Monica Alexander’s <a href="https://www.monicaalexander.com/posts/2020-28-02-bayes_viz/">example</a> of visualisation in an end-to-end bayesian analysis. I’ve included a full list of resources which helped me at the end of this post. The steps in bold will be discussed below while an application using the full workflow will be in an upcoming writeup.</p>
<section id="steps-in-proposed-workflow" class="level3">
<h3 class="anchored" data-anchor-id="steps-in-proposed-workflow"><strong>Steps in proposed workflow</strong></h3>
<ol type="1">
<li><p><strong>Exploratory data analysis and data transformation</strong></p></li>
<li><p><strong>Write out full probability model</strong></p></li>
<li><p><strong>Prior predictive checks - simulate data from the implied generative model</strong></p></li>
<li><p><strong>Fit model on fake data - can we recover the known parameters?</strong></p></li>
<li><p><strong>Estimate model on real data</strong></p></li>
<li><p>Check whether MCMC sampler ran efficiently and model convergence</p></li>
<li><p><strong>Posterior predictive check to evaluate model fit</strong></p></li>
<li><p>Model selection using cross validation and information criteria</p></li>
<li><p><em>Optional:</em> Evaluate model performance on test or out of sample dataset (not strictly necessary if the modelling task is not a predictive problem)</p></li>
</ol>
</section>
</section>
<section id="predicting-housing-prices" class="level1">
<h1>Predicting housing prices</h1>
<section id="pick-your-language" class="level2">
<h2 class="anchored" data-anchor-id="pick-your-language">Pick your language</h2>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">R</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<section id="exploratory-data-analysis-and-data-transformation" class="level3">
<h3 class="anchored" data-anchor-id="exploratory-data-analysis-and-data-transformation">1) Exploratory data analysis and data transformation</h3>
<p>The full dataset for this competition contains 79 features to predict the target variable <code>SalesPrice</code>. For this exercise I will focus on two variables: <code>Neighbourhood</code> (categorical: physical locations within Ames city limits) and <code>LotArea</code> (positive real: lot size in square feet). I chose these variables as they are consistent with my understanding of how housing prices vary in relation to their location and property size.</p>
<p>Aside: The model and feature selection in this example are deliberately simple. The goal is to motivate workflow, diagnostics and to interrogate assumptions, so I only used two variables to make it easier to follow. My <a href="https://github.com/bennywee/house_prices_kaggle">repo</a> contains examples of other models and additional features.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pystan</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> arviz <span class="im">as</span> az</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>az.style.use(<span class="st">'arviz-darkgrid'</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'data/train.csv'</span>).loc[:, [<span class="st">'SalePrice'</span>, <span class="st">'LotArea'</span>, <span class="st">'Neighborhood'</span>]]</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Log transform</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'log_sales_price'</span>] <span class="op">=</span> np.log(df[<span class="st">'SalePrice'</span>])</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'log_lot_area'</span>] <span class="op">=</span> np.log(df[<span class="st">'LotArea'</span>])</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create numerical categories (add 1 due to zero indexing)</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'neighbourhood'</span>] <span class="op">=</span> df[<span class="st">'Neighborhood'</span>].astype(<span class="st">'category'</span>).cat.codes<span class="op">+</span><span class="dv">1</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>df.head().style</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">

<style type="text/css">
</style>
<table id="T_e3be0">
  <thead>
    <tr>
      <th class="blank level0">&nbsp;</th>
      <th id="T_e3be0_level0_col0" class="col_heading level0 col0">SalePrice</th>
      <th id="T_e3be0_level0_col1" class="col_heading level0 col1">LotArea</th>
      <th id="T_e3be0_level0_col2" class="col_heading level0 col2">Neighborhood</th>
      <th id="T_e3be0_level0_col3" class="col_heading level0 col3">log_sales_price</th>
      <th id="T_e3be0_level0_col4" class="col_heading level0 col4">log_lot_area</th>
      <th id="T_e3be0_level0_col5" class="col_heading level0 col5">neighbourhood</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_e3be0_level0_row0" class="row_heading level0 row0">0</th>
      <td id="T_e3be0_row0_col0" class="data row0 col0">208500</td>
      <td id="T_e3be0_row0_col1" class="data row0 col1">8450</td>
      <td id="T_e3be0_row0_col2" class="data row0 col2">CollgCr</td>
      <td id="T_e3be0_row0_col3" class="data row0 col3">12.247694</td>
      <td id="T_e3be0_row0_col4" class="data row0 col4">9.041922</td>
      <td id="T_e3be0_row0_col5" class="data row0 col5">6</td>
    </tr>
    <tr>
      <th id="T_e3be0_level0_row1" class="row_heading level0 row1">1</th>
      <td id="T_e3be0_row1_col0" class="data row1 col0">181500</td>
      <td id="T_e3be0_row1_col1" class="data row1 col1">9600</td>
      <td id="T_e3be0_row1_col2" class="data row1 col2">Veenker</td>
      <td id="T_e3be0_row1_col3" class="data row1 col3">12.109011</td>
      <td id="T_e3be0_row1_col4" class="data row1 col4">9.169518</td>
      <td id="T_e3be0_row1_col5" class="data row1 col5">25</td>
    </tr>
    <tr>
      <th id="T_e3be0_level0_row2" class="row_heading level0 row2">2</th>
      <td id="T_e3be0_row2_col0" class="data row2 col0">223500</td>
      <td id="T_e3be0_row2_col1" class="data row2 col1">11250</td>
      <td id="T_e3be0_row2_col2" class="data row2 col2">CollgCr</td>
      <td id="T_e3be0_row2_col3" class="data row2 col3">12.317167</td>
      <td id="T_e3be0_row2_col4" class="data row2 col4">9.328123</td>
      <td id="T_e3be0_row2_col5" class="data row2 col5">6</td>
    </tr>
    <tr>
      <th id="T_e3be0_level0_row3" class="row_heading level0 row3">3</th>
      <td id="T_e3be0_row3_col0" class="data row3 col0">140000</td>
      <td id="T_e3be0_row3_col1" class="data row3 col1">9550</td>
      <td id="T_e3be0_row3_col2" class="data row3 col2">Crawfor</td>
      <td id="T_e3be0_row3_col3" class="data row3 col3">11.849398</td>
      <td id="T_e3be0_row3_col4" class="data row3 col4">9.164296</td>
      <td id="T_e3be0_row3_col5" class="data row3 col5">7</td>
    </tr>
    <tr>
      <th id="T_e3be0_level0_row4" class="row_heading level0 row4">4</th>
      <td id="T_e3be0_row4_col0" class="data row4 col0">250000</td>
      <td id="T_e3be0_row4_col1" class="data row4 col1">14260</td>
      <td id="T_e3be0_row4_col2" class="data row4 col2">NoRidge</td>
      <td id="T_e3be0_row4_col3" class="data row4 col3">12.429216</td>
      <td id="T_e3be0_row4_col4" class="data row4 col4">9.565214</td>
      <td id="T_e3be0_row4_col5" class="data row4 col5">16</td>
    </tr>
  </tbody>
</table>

</div>
</div>
<p>A scatter plot shows a positive correlation between <code>log(SalePrice)</code> and <code>log(LotArea)</code>. Fitting OLS on the logarithms of both variables assumes a linear relationship on the multiplicative scale. All else equal, property prices tend to be higher with larger lot sizes. However, this univariate linear model clearly underfits the data and there are almost surely unobserved confounding variables.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>sns.lmplot(x<span class="op">=</span><span class="st">'log_lot_area'</span>,y<span class="op">=</span><span class="st">'log_sales_price'</span>,data<span class="op">=</span>df,fit_reg<span class="op">=</span><span class="va">True</span>, ci <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/2_pooling_scatter.svg" class="img-fluid">
</center>
<p>A potential reason for underfitting may be some neighbourhoods have higher average prices than other neighbourhoods (which would result in different intercepts). Furthermore, the <em>association</em> between housing prices and lot size may depend on different neighbourhoods as well (varying slopes). This variation could be driven by different zonings or housing densities within neighbourhoods that could impact the relationship between lot size and prices. Splitting the plot out by neighbourhood displays the heterogeneity in linear trends.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>facet_scatter <span class="op">=</span> sns.lmplot(x<span class="op">=</span><span class="st">"log_lot_area"</span>, </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>                           y<span class="op">=</span><span class="st">"log_sales_price"</span>, </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>                           col<span class="op">=</span><span class="st">"Neighborhood"</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>                           col_wrap <span class="op">=</span> <span class="dv">5</span>,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>                           data<span class="op">=</span>df,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>                           ci <span class="op">=</span> <span class="va">None</span>, </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>                           truncate<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>                           col_order <span class="op">=</span> <span class="bu">sorted</span>(df[<span class="st">'Neighborhood'</span>].drop_duplicates()))</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>facet_scatter.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">15</span>))</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>facet_scatter.<span class="bu">set</span>(ylim<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">15</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/3_facet_scatter.svg" class="img-fluid">
</center>
<p>We can see variation in the slopes and intercepts as well as imbalanced sampling between neighbourhood clusters. This and other unobserved confounders probably contributed to some of the weak/negative gradients. The small sample sizes in some neighbourhoods will be prone to overfitting and will give noisy estimates which will require regularisation.</p>
</section>
<section id="write-out-full-probability-model" class="level3">
<h3 class="anchored" data-anchor-id="write-out-full-probability-model">2) Write out full probability model</h3>
<p>3 basic linear models can be used to approach this problem:</p>
<ol type="1">
<li>Pooled OLS (assumes all observations come from “one neighbourhood”, equivalent to the OLS model in the first scatterplot)</li>
<li>No pooling OLS (conceptually the same as a dummy variable regression - assumes independence between all neighbourhoods)</li>
<li>Saturated regression (adds interactive effects between <code>log(LotArea)_i</code> and <code>neighbourhood</code> to no pooling OLS)</li>
</ol>
<p>I will use no pooling OLS to demonstrate the rest of the workflow. There is definitely room for improving these models. In fact, this problem is a good candidate for multilevel models. They allow for information to be borrowed between neighbourhood clusters through partial pooling and removes the assumption of independence. This will help regularise the effects of small and imbalanced sample sizes across <code>neighbourhood</code>. I will apply the full workflow using multilevel models in the next post.</p>
<section id="model-specification" class="level4">
<h4 class="anchored" data-anchor-id="model-specification">Model specification</h4>
<p>The no pooling regression is written out below, where <span class="math inline">\(i\)</span> indexes the property and <span class="math inline">\(j\)</span> indexes each neighbourhood. I’ve assigned a gaussian likelihood which assumes that the residuals are normally distributed.</p>
<p><span class="math display">\[
y_i \sim Normal(\mu_i, \sigma) \\
\\
\mu_i = \alpha_{j} + \beta * x_i \\
\]</span> Where <span class="math inline">\(y_i\)</span> is <code>log(SalesPrice)</code> and <span class="math inline">\(x_i\)</span> is <code>log(LotArea)</code> scaled to mean 0 and standard deviation 1. <span class="math inline">\(\alpha_j\)</span> is an intercept parameter for the jth neighbourhood in the sample. The slope coefficient can be interpreted as: a one standard deviation increase in <code>log(LotArea)</code> is a <span class="math inline">\(\beta\)</span> standard deviation change in <code>log(SalesPrice)</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardise predictors</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> z_std(x):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Centres at mean 0 and standard deviation 1</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    z_score <span class="op">=</span> (x <span class="op">-</span> x.mean()) <span class="op">/</span> x.std()</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(z_score)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Center and scale predictor</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'log_lot_area_z'</span>] <span class="op">=</span> z_std(df[<span class="st">'log_lot_area'</span>])</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale target</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'log_sales_price_z'</span>] <span class="op">=</span> z_std(df[<span class="st">'log_sales_price'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><span class="math display">\[
y_i = \frac{log(SalesPrice)_i - \overline{log(SalesPrice)}}{\sigma_{log(SalesPrice)}} \\
x_i = \frac{log(LotArea)_i - \overline{log(LotArea)}}{\sigma_{log(LotArea)}}
\]</span></p>
<p>Standardising both outcome and predictor variables makes sampling from the posterior distribution easier when we fit the model. If we had more continuous regressors, we could also compare the parameters on the same scale. Standardising also plays an important role in setting priors as we’ll see below.</p>
</section>
<section id="selecting-priors" class="level4">
<h4 class="anchored" data-anchor-id="selecting-priors">Selecting priors</h4>
<p>Probability distributions need to be assigned to the parameters for this to be a bayesian model. Setting priors is an opportunity to encode domain knowledge or results from related studies into the model. Unfortunately, I do not have much domain expertise or information about the context of this dataset to give very informative priors. So I have chosen to use weakly informative priors following the advice of the <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">Stan developers</a>. This will help me regularise model predictions within the plausible outcome space.</p>
<p>For <span class="math inline">\(\beta\)</span> I’ll assign a <span class="math inline">\(Normal(0, 1)\)</span> which puts ~95% of the probability between two standard deviations for a unit increase in <span class="math inline">\(x\)</span>. We want to hedge against overfitting by shrinking the coefficient towards zero. This is achieved by putting probability mass on all plausible values of <span class="math inline">\(\beta\)</span> with less weight on extreme relationships.</p>
<p><span class="math inline">\(\alpha_j\)</span> is the intercept for the <span class="math inline">\(j^{th}\)</span> neighbourhood. In a pooled OLS regression between price and lot area, the intercept <span class="math inline">\(\alpha\)</span> (ignoring the neighbourhood means ignoring the j subscript) would be interpreted as the value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x\)</span> is 0. Since <span class="math inline">\(x\)</span> has a mean of zero, <span class="math inline">\(\alpha\)</span> has the additional interpretation as the value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x\)</span> is equal to its sample mean. By construction, <span class="math inline">\(\alpha\)</span> must be 0, the sample mean of <span class="math inline">\(y\)</span>.</p>
<p>So in the case of <span class="math inline">\(\alpha_j\)</span> I set a normal prior with a mean of 0 and a standard deviation of 1 for all neighbourhoods, regularising neighbourhood effects within two standard deviations of the grand mean of <span class="math inline">\(y\)</span>.</p>
<p>The variance parameter <span class="math inline">\(\sigma\)</span> is defined over positive real numbers. So our prior should only put probabilistic weight on positive values. In this case I’ve chosen a weakly regularising <span class="math inline">\(exponential(1)\)</span> prior. Other candidate priors are the Half-Cauchy distribution or the Half-Normal which has thinner tails.</p>
<p>These weakly informative priors express my belief that the parameters of this model would overfit the sample and that we need to regularise their effects. Standardising the variables made this job much easier and intuitive. All together the full model looks like:</p>
<p><span class="math display">\[
y_i \sim Normal(\mu_i, \sigma) \\
\mu_i = \alpha_{j} + \beta * x_i \\
\alpha_j\sim Normal(0, 1)\\
\beta\sim Normal(0, 1) \\
\sigma\sim exp(1)
\]</span></p>
</section>
</section>
<section id="prior-predictive-checks---simulate-data-from-the-implied-generative-model" class="level3">
<h3 class="anchored" data-anchor-id="prior-predictive-checks---simulate-data-from-the-implied-generative-model">3) Prior predictive checks - simulate data from the implied generative model</h3>
<p>Prior predictive checks are useful for understanding the implications of our priors. Parameters are simulated from the joint prior distribution and visualised to see the implied relationships between the target and predictor variables. This will help diagnose any problems with our assumptions and modelling decisions. These checks become more important for generalised linear models since the outcome and parameter space are different due to the link function. For example, priors on parameters in a logistic regression are in the <em>log-odds space</em> and may behave differently to our expectations on the <em>outcome space</em>.</p>
<p>The code below includes all the inputs necessary to estimate the model on the data. Setting <code>run_estimation = 0</code> means Stan will only simulate values from the joint prior distribution since the likelihood is not evaluated (thanks to Jim for this handy <a href="https://khakieconomics.github.io/2017/04/30/An-easy-way-to-simulate-fake-data-in-stan.html">tip</a>).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>no_pooling_stan_code <span class="op">=</span> <span class="st">'''</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="st">// No pooling model for predicting housing prices</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="st">data {</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="st">    // Fitting the model on training data</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="st">    int&lt;lower=0&gt; N; // Number of rows</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="st">    int&lt;lower=0&gt; neighbourhood[N]; // neighbourhood categorical variable</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="st">    int&lt;lower=0&gt; N_neighbourhood; // number of neighbourhood categories</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="st">    vector[N] log_sales_price; // log sales price</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="st">    vector[N] log_lot_area; // log lot area</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="st">    // Adjust scale parameters in python</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="st">    real alpha_sd;</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="st">    real beta_sd;</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="st">    // Set to zero for prior predictive checks, set to one to evaluate likelihood</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="st">    int&lt;lower = 0, upper = 1&gt; run_estimation;</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="st">parameters {</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="st">    vector[N_neighbourhood] alpha; // Vector of alpha coefficients for each neighbourhood</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="st">    real beta;</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="st">    real&lt;lower=0&gt; sigma;</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="st">model {</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="st">    // Priors</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="st">    target += normal_lpdf(alpha | 0, alpha_sd);</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="st">    target += normal_lpdf(beta | 0, beta_sd);</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="st">    target += exponential_lpdf(sigma |1);</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="st">    // Likelihood</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="st">    if(run_estimation==1){</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a><span class="st">        target += normal_lpdf(log_sales_price | alpha[neighbourhood] + beta * log_lot_area, sigma);</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="st">    }</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a><span class="st">generated quantities {</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="st">    // Uses fitted model to generate values of interest</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a><span class="st">    vector[N] log_lik; // Log likelihood</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="st">    vector[N] y_hat; // Predictions using training data</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a><span class="st">    {</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a><span class="st">    for(n in 1:N){</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a><span class="st">          log_lik[n] = normal_lpdf(log_sales_price | alpha[neighbourhood[n]] + beta * log_lot_area[n], sigma);</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a><span class="st">          y_hat[n] = normal_rng(alpha[neighbourhood[n]] + beta * log_lot_area[n], sigma);      </span></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a><span class="st">        }</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="st">    }</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a><span class="st">'''</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Dictionary contains all data inputs</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>npm_data_check <span class="op">=</span> <span class="bu">dict</span>(N <span class="op">=</span> <span class="bu">len</span>(df),</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>                      log_sales_price <span class="op">=</span> df[<span class="st">'log_sales_price_z'</span>],</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>                      log_lot_area <span class="op">=</span> df[<span class="st">'log_lot_area_z'</span>],</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>                      neighbourhood <span class="op">=</span> df[<span class="st">'neighbourhood'</span>],</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>                      N_neighbourhood <span class="op">=</span> <span class="bu">len</span>(df[<span class="st">'neighbourhood'</span>].unique()),</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>                      alpha_sd <span class="op">=</span> <span class="dv">1</span>, </span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>                      beta_sd <span class="op">=</span> <span class="dv">1</span>, </span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>                      run_estimation <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile stan model</span></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>no_pooling_model <span class="op">=</span> pystan.StanModel(model_code <span class="op">=</span> no_pooling_stan_code)</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw samples from joint prior distribution</span></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>fit_npm_check <span class="op">=</span> no_pooling_model.sampling(data<span class="op">=</span>npm_data_check, seed <span class="op">=</span> <span class="dv">12345</span>)</span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract samples into a pandas dataframe</span></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>npm_df_check <span class="op">=</span> fit_npm_check.to_dataframe()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<blockquote class="blockquote">
<p>For the prior predictive checks, we recommend not cleaving too closely to the observed data and instead aiming for a prior data generating process that can produce plausible data sets, not necessarily ones that are indistinguishable from observed data. - <a href="https://arxiv.org/abs/1709.01449">Visualisation Bayesian Workflow - Gabry, Simpson, Vehtari, Betancourt, Gelman (2019)</a></p>
</blockquote>
<p>The implied predictions of our priors are visualised below. I’ve arbitrarily chosen the 4th neighbourhood index (<span class="math inline">\(\alpha_{j=4}\)</span>) since the priors for the neighbourhoods are the same. Weakly informative priors should create bounds between possible values while allowing for some implausible relationships. Remembering that 95% of gaussian mass exists within two standard deviations of the mean is a useful guide for determining what is reasonable.</p>
<p>Let’s see an example of setting uninformative priors and its implications of the data generating process. I’ve set the scale parameters for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> to be 10 which are quite diffuse. The implied predictions of the mean are much wider and well beyond the minimum and maximum values in the real data. This suggests that the model is giving too much probabilistic weight to highly implausible datasets.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>npm_data_check_wide <span class="op">=</span> <span class="bu">dict</span>(N <span class="op">=</span> <span class="bu">len</span>(df),</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>                      log_sales_price <span class="op">=</span> df[<span class="st">'log_sales_price_z'</span>],</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>                      log_lot_area <span class="op">=</span> df[<span class="st">'log_lot_area_z'</span>],</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>                      neighbourhood <span class="op">=</span> df[<span class="st">'neighbourhood'</span>],</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>                      N_neighbourhood <span class="op">=</span> <span class="bu">len</span>(df[<span class="st">'Neighborhood'</span>].unique()),</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>                      alpha_sd <span class="op">=</span> <span class="dv">10</span>, </span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>                      beta_sd <span class="op">=</span> <span class="dv">10</span>, </span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>                      run_estimation <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>fit_npm_check_wide <span class="op">=</span> no_pooling_model.sampling(data<span class="op">=</span>npm_data_check_wide)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>npm_df_check_wide <span class="op">=</span> fit_npm_check_wide.to_dataframe()</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>_, ax <span class="op">=</span> plt.subplots(figsize <span class="op">=</span> (<span class="dv">13</span>, <span class="dv">8</span>))</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">200</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha, beta <span class="kw">in</span> <span class="bu">zip</span>(npm_df_check_wide[<span class="st">"alpha[4]"</span>][:<span class="dv">100</span>], npm_df_check_wide[<span class="st">"beta"</span>][:<span class="dv">100</span>]):</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> alpha <span class="op">+</span> beta <span class="op">*</span> x</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, y, c<span class="op">=</span><span class="st">"k"</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"x (z-scores)"</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Fitted y (z-scores)"</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Prior predictive checks -- Uninformative (flat) priors"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/5_prior_predictive_check_wide.svg" class="img-fluid">
</center>
<p>Our original scale parameters of 1 produce more reasonable relationships. There are still some extreme regression lines implied by our data generating process, but they are bound to more realistic outcomes relative to the diffuse priors.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>_, ax <span class="op">=</span> plt.subplots(figsize <span class="op">=</span> (<span class="dv">13</span>, <span class="dv">8</span>))</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">200</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha, beta <span class="kw">in</span> <span class="bu">zip</span>(npm_df_check[<span class="st">"alpha[4]"</span>][:<span class="dv">100</span>], npm_df_check[<span class="st">"beta"</span>][:<span class="dv">100</span>]):</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> alpha <span class="op">+</span> beta <span class="op">*</span> x</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, y, c<span class="op">=</span><span class="st">"blue"</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"x (z-scores)"</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Fitted y (z-scores)"</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Prior predictive checks -- Weakly regularizing priors"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/4_prior_predictive_check.svg" class="img-fluid">
</center>
<p>Putting both sets of lines on the same scale emphasises the difference in simulated values. The blue lines from the previous graph cover a tighter space relative to the simulations from the uninformative priors.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Putting both on the same scale</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>_, ax <span class="op">=</span> plt.subplots(figsize <span class="op">=</span> (<span class="dv">13</span>, <span class="dv">8</span>))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha_wide, beta_wide <span class="kw">in</span> <span class="bu">zip</span>(npm_df_check_wide[<span class="st">"alpha[4]"</span>][:<span class="dv">100</span>], npm_df_check_wide[<span class="st">"beta"</span>][:<span class="dv">100</span>]):</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    y_wide <span class="op">=</span> alpha_wide <span class="op">+</span> beta_wide <span class="op">*</span> x</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, y_wide, c<span class="op">=</span><span class="st">"k"</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha, beta <span class="kw">in</span> <span class="bu">zip</span>(npm_df_check[<span class="st">"alpha[4]"</span>][:<span class="dv">100</span>], npm_df_check[<span class="st">"beta"</span>][:<span class="dv">100</span>]):</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> alpha <span class="op">+</span> beta <span class="op">*</span> x</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, y, c<span class="op">=</span><span class="st">"blue"</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"x (z-scores)"</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Fitted y (z-scores)"</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Prior predictive checks -- Uninformative (black) vs weakly informative (blue)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/5_prior_predictive_check_compare.svg" class="img-fluid">
</center>
</section>
<section id="fit-model-on-fake-data" class="level3">
<h3 class="anchored" data-anchor-id="fit-model-on-fake-data">4) Fit model on fake data</h3>
<p>We can use the simulations to see if our model can successfully estimate the parameters used to generate fake data (the implied <span class="math inline">\(\hat{y}\)</span>). Take a draw from the prior samples (e.g.&nbsp;the 50th simulation) and estimate the model on the data produced by these parameters. Let’s see if the model fitted on fake data can capture the “true” parameters (dotted red lines) of the data generating process. If the model cannot capture the <em>known</em> parameters which generated fake data, there is no certainty it will be estimating the correct parameters on real data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pick random simulation, let's say 50</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>random_draw <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the simulated (fake) data implied by the parameters in sample 50</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>y_sim <span class="op">=</span> npm_df_check.<span class="bu">filter</span>(regex <span class="op">=</span> <span class="st">'y_hat'</span>).iloc[random_draw, :]</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the parameters corresponding to sample 10</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>true_parameters <span class="op">=</span> npm_df_check.<span class="bu">filter</span>(regex <span class="op">=</span> <span class="st">'alpha|beta|sigma'</span>).iloc[random_draw, :]</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model on the fake data</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>_npm_data_check <span class="op">=</span> <span class="bu">dict</span>(N <span class="op">=</span> <span class="bu">len</span>(df),</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>              log_sales_price <span class="op">=</span> y_sim, <span class="co"># this is now fitting on the extracted fake data in sample 50</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>              log_lot_area <span class="op">=</span> df[<span class="st">'log_lot_area_z'</span>],</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>              neighbourhood <span class="op">=</span> df[<span class="st">'neighbourhood'</span>],</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>              N_neighbourhood <span class="op">=</span> <span class="bu">len</span>(df[<span class="st">'Neighborhood'</span>].unique()),</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>              alpha_sd <span class="op">=</span> <span class="dv">1</span>, </span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>              beta_sd <span class="op">=</span> <span class="dv">1</span>, </span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>              run_estimation <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>_fit_npm_check <span class="op">=</span> no_pooling_model.sampling(data<span class="op">=</span>_npm_data_check, seed <span class="op">=</span> <span class="dv">12345</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>_npm_df_check <span class="op">=</span> _fit_npm_check.to_dataframe()</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>fake_fit <span class="op">=</span> _npm_df_check.<span class="bu">filter</span>(regex <span class="op">=</span> <span class="st">'alpha|beta|sigma'</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>parameter_df <span class="op">=</span> pd.melt(fake_fit)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot will give distributions of all parameters to see if it can capture the known parameters</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="bu">max</span>(<span class="dv">2</span>, math.ceil(fake_fit.shape[<span class="dv">1</span>] <span class="op">/</span> <span class="dv">6</span>)), ncols<span class="op">=</span><span class="dv">6</span>, sharex<span class="op">=</span><span class="va">False</span>, sharey <span class="op">=</span> <span class="va">False</span>, figsize<span class="op">=</span>(<span class="dv">21</span>,<span class="dv">13</span>))</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'Model Checking - red lines are "true" parameters'</span>, size <span class="op">=</span> <span class="dv">30</span>)</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>axes_list <span class="op">=</span> [item <span class="cf">for</span> sublist <span class="kw">in</span> axes <span class="cf">for</span> item <span class="kw">in</span> sublist] </span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>parameters <span class="op">=</span> parameter_df[[<span class="st">'variable'</span>]].drop_duplicates().set_index(<span class="st">'variable'</span>).index</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>grouped <span class="op">=</span> parameter_df.groupby(<span class="st">"variable"</span>)</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> parameter <span class="kw">in</span> parameters:</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>    selection <span class="op">=</span> grouped.get_group(parameter)</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes_list.pop(<span class="dv">0</span>)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>    selection.plot.kde(label<span class="op">=</span>parameter, ax<span class="op">=</span>ax, legend<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>    ax.set_title(parameter)</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>    ax.grid(linewidth<span class="op">=</span><span class="fl">0.25</span>)</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>    ax.axvline(x<span class="op">=</span>true_parameters[parameter], color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha <span class="op">=</span> <span class="fl">0.5</span>)</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Now use the matplotlib .remove() method to delete anything we didn't use</span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axes_list:</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>    ax.remove()</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/6_fit_fake_data.svg" class="img-fluid">
</center>
<p>The model sufficiently captured the known parameters. The next post will go through a more interesting example where this fails and requires us to rethink how we specified our models.</p>
</section>
<section id="estimate-model-on-real-data" class="level3">
<h3 class="anchored" data-anchor-id="estimate-model-on-real-data">5) Estimate model on real data</h3>
<p>Set <code>run_estimation=1</code> and run the code to fit the model. Stan will sample the joint posterior distribution using the default Markov chain Monte Carlo (MCMC) algorithm, the <a href="https://mc-stan.org/docs/2_18/reference-manual/hmc-algorithm-parameters.html">No-U-Turn sampler (NUTs)</a>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dictionary with data inputs - set run_estimation=1</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>npm_data <span class="op">=</span> <span class="bu">dict</span>(N <span class="op">=</span> <span class="bu">len</span>(df),</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>              log_sales_price <span class="op">=</span> df[<span class="st">'log_sales_price_z'</span>],</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>              log_lot_area <span class="op">=</span> df[<span class="st">'log_lot_area_z'</span>],</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>              neighbourhood <span class="op">=</span> df[<span class="st">'neighbourhood'</span>],</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>              N_neighbourhood <span class="op">=</span> <span class="bu">len</span>(df[<span class="st">'Neighborhood'</span>].unique()),</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>              alpha_sd <span class="op">=</span> <span class="dv">1</span>, </span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>              beta_sd <span class="op">=</span> <span class="dv">1</span>, </span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>              run_estimation <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model by sampling from posterior distribution</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>fit_npm <span class="op">=</span> no_pooling_model.sampling(data<span class="op">=</span>npm_data)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co"># For generating visualisations using the arviz package</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>npm_az <span class="op">=</span> az.from_pystan(</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    posterior<span class="op">=</span>fit_npm,</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    posterior_predictive<span class="op">=</span><span class="st">"y_hat"</span>,</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    observed_data<span class="op">=</span><span class="st">"log_sales_price"</span>,</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    log_likelihood<span class="op">=</span><span class="st">'log_lik'</span>,</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract samples into dataframe</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>fit_npm_df <span class="op">=</span> fit_npm.to_dataframe()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="check-whether-mcmc-sampler-and-model-fit" class="level3">
<h3 class="anchored" data-anchor-id="check-whether-mcmc-sampler-and-model-fit">6) Check whether MCMC sampler and model fit<!--{.tabset .tabset-fade .tabset-pills}--></h3>
<p>Stan won’t have trouble sampling from such a simple model, so I won’t go through chain diagnostics in detail. I’ve included number of effective samples and Rhat diagnostics for completeness. We can see the posterior distributions of all the parameters by looking at the traceplot as well.</p>
<section id="traceplot" class="level4">
<h4 class="anchored" data-anchor-id="traceplot">Traceplot</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect model fit</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>az.plot_trace(fit_npm, </span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>              var_names<span class="op">=</span>[<span class="st">"alpha"</span>, <span class="st">"beta"</span>, <span class="st">"sigma"</span>], </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>              compact <span class="op">=</span> <span class="va">True</span>, </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>              chain_prop <span class="op">=</span> <span class="st">'color'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/7_trace_plot.png" class="img-fluid">
</center>
</section>
<section id="posterior-distributions" class="level4">
<h4 class="anchored" data-anchor-id="posterior-distributions">Posterior distributions</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect model fit</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> az.plot_forest(fit_npm, </span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>              var_names<span class="op">=</span>[<span class="st">"alpha"</span>, <span class="st">"beta"</span>, <span class="st">"sigma"</span>],</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>              combined <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Posterior distributions of fitted parameters'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/7_posterior.svg" class="img-fluid">
</center>
</section>
<section id="neff-rhat" class="level4">
<h4 class="anchored" data-anchor-id="neff-rhat">neff / Rhat</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pystan.stansummary(fit_npm, </span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>                         pars<span class="op">=</span>[<span class="st">'alpha'</span>, <span class="st">'beta'</span>, <span class="st">'sigma'</span>], </span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>                         probs<span class="op">=</span>(<span class="fl">0.025</span>, <span class="fl">0.50</span>, <span class="fl">0.975</span>), </span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>                         digits_summary<span class="op">=</span><span class="dv">3</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<pre><code>Inference for Stan model: anon_model_9d4f76eb27d91c6b75464a26e0b032c7.
4 chains, each with iter=2000; warmup=1000; thin=1;
post-warmup draws per chain=1000, total post-warmup draws=4000.

            mean se_mean     sd   2.5%    50%  97.5%  n_eff   Rhat
alpha[1]   1.004   0.002  0.148   0.71  1.004  1.288   6313  0.999
alpha[2]   0.565   0.005  0.401 -0.211  0.564  1.334   6117  0.999
alpha[3]  -0.102   0.002  0.162 -0.412 -0.102  0.212   4685    1.0
alpha[4]  -0.686 9.71e-4  0.082 -0.845 -0.686 -0.523   7060    1.0
alpha[5]   0.003   0.002  0.121 -0.2292.25e-4  0.245   6299    1.0
alpha[6]    0.33 5.69e-4  0.051  0.229   0.33  0.429   7894  0.999
alpha[7]    0.34   0.001  0.087  0.169  0.339  0.507   7475    1.0
alpha[8]   -0.78 6.89e-4  0.059 -0.895 -0.781 -0.664   7357    1.0
alpha[9]   0.215 9.01e-4  0.068  0.082  0.215   0.35   5743  0.999
alpha[10] -1.328   0.001  0.101 -1.525 -1.329 -1.132   7365    1.0
alpha[11]  -0.41   0.002  0.159 -0.715 -0.408 -0.105   5670    1.0
alpha[12] -0.319 9.71e-4  0.087 -0.496  -0.32 -0.142   8023  0.999
alpha[13]  -0.44 4.84e-4  0.041  -0.52  -0.44 -0.362   7346  0.999
alpha[14]  0.312   0.002  0.202 -0.087  0.313  0.714   7154  0.999
alpha[15]    0.1 8.56e-4  0.071  -0.04    0.1   0.24   6834  0.999
alpha[16]   1.37   0.001  0.095  1.181  1.369  1.561   6408    1.0
alpha[17]  1.412 8.08e-4  0.068  1.277  1.412  1.546   7174  0.999
alpha[18] -0.685 6.69e-4  0.057 -0.797 -0.685  -0.57   7138  0.999
alpha[19] -0.362   0.001  0.122 -0.598 -0.362 -0.127   7838    1.0
alpha[20] -0.597 7.55e-4  0.071 -0.733 -0.597 -0.456   8863  0.999
alpha[21]  0.121  9.7e-4  0.079 -0.036   0.12  0.274   6678  0.999
alpha[22]  0.869 7.96e-4  0.066  0.739  0.869  0.995   6790    1.0
alpha[23]   1.42   0.001  0.123  1.176  1.421  1.659   7563    1.0
alpha[24]  0.503   0.001  0.099   0.31  0.501  0.699   7092    1.0
alpha[25]  0.515   0.002  0.179  0.167  0.515  0.868   7366  0.999
beta       0.347 3.71e-4  0.021  0.307  0.347  0.387   3126  0.999
sigma      0.607 1.37e-4  0.011  0.586  0.607  0.629   6687  0.999

Samples were drawn using NUTS at Thu Nov 12 11:48:18 2020.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
</section>
</section>
<section id="posterior-predictive-check-to-evaluate-model-performance" class="level3">
<h3 class="anchored" data-anchor-id="posterior-predictive-check-to-evaluate-model-performance">7) Posterior predictive check to evaluate model performance</h3>
<p>How well did the model perform? We can perform posterior predictive checks to see if sampled distributions from the fitted model can approximate the density of <code>SalesPrice</code>. If the model performs well, it should be able to retrodict the density of the data used to train the model. The blue lines are the predictions drawn from the joint posterior distribution compared with the observed density of the target <span class="math inline">\(y\)</span> variable.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Select 300 samples to plot against observed distribution</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>az.plot_ppc(data <span class="op">=</span> npm_az, </span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>            kind <span class="op">=</span> <span class="st">'kde'</span>, </span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>            data_pairs <span class="op">=</span> {<span class="st">'log_sales_price'</span> : <span class="st">'y_hat'</span>},</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>            legend <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>            color<span class="op">=</span><span class="st">'cyan'</span>,</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>            mean <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>            figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>),</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>            alpha <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>            num_pp_samples<span class="op">=</span><span class="dv">300</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/8_posterior_predictive_check.png" class="img-fluid">
</center>
<p>Reversing the data transformations gives back the posterior predictive checks on the natural scale (rescale <span class="math inline">\(y\)</span> and exponentiate <code>log(SalesPrice)</code> to get back <code>SalesPrice</code>):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> pl<span class="co">#| t.subplots(1,1, figsize = (13, 8))</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>np.exp(fit_npm_df.<span class="bu">filter</span>(regex <span class="op">=</span> <span class="st">'y_hat'</span>)<span class="op">*</span>df[<span class="st">'log_sales_price'</span>].std()<span class="op">+</span>df[<span class="st">'log_sales_price'</span>].mean())<span class="op">\</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>                 .T<span class="op">\</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>                 .iloc[:, :<span class="dv">300</span>]<span class="op">\</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>                 .plot.kde(legend <span class="op">=</span> <span class="va">False</span>, </span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>                           title <span class="op">=</span> <span class="st">'Posterior predictive Checks - Black: Observed Sale Price, blue: posterior samples'</span>, </span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>                           xlim <span class="op">=</span> (<span class="dv">30000</span>,<span class="dv">500000</span>),</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>                           alpha <span class="op">=</span> <span class="fl">0.08</span>,</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>                           ax <span class="op">=</span> axes, color <span class="op">=</span> <span class="st">'aqua'</span>)<span class="op">;</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'SalePrice'</span>].plot.kde(legend <span class="op">=</span> <span class="va">False</span>, </span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>                         xlim <span class="op">=</span> (<span class="dv">30000</span>,<span class="dv">500000</span>),</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>                         alpha <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>                         ax <span class="op">=</span> axes,</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>                         color <span class="op">=</span> <span class="st">'black'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/9_posterior_predictive_check_outcomescale.png" class="img-fluid">
</center>
<p>Not bad for a simple model. There is definitely room for iteration and improvement.</p>
</section>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<section id="exploratory-data-analysis-and-data-transformation-1" class="level3">
<h3 class="anchored" data-anchor-id="exploratory-data-analysis-and-data-transformation-1">1) Exploratory data analysis and data transformation</h3>
<p>The full dataset for this competition contains 79 features to predict the target variable <code>SalesPrice</code>. For this exercise I will focus on two variables: <code>Neighbourhood</code> (categorical: physical locations within Ames city limits) and <code>LotArea</code> (positive real: lot size in square feet). I chose these variables as they are consistent with my understanding of how housing prices vary in relation to their location and property size.</p>
<p>Aside: The model and feature selection in this example are deliberately simple. The goal is to motivate workflow, diagnostics and to interrogate assumptions, so I only used two variables to make it easier to follow. My <a href="https://github.com/bennywee/house_prices_kaggle">repo</a> contains examples of other models and additional features.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rstan)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(bayesplot)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Detect cores for parallel sampling</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">mc.cores =</span> parallel<span class="sc">::</span><span class="fu">detectCores</span>())</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data, select variables, apply log transformation</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span>  <span class="fu">read_csv</span>(<span class="st">'data/train.csv'</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="st">'SalePrice'</span>, <span class="st">'LotArea'</span>, <span class="st">'Neighborhood'</span>)<span class="sc">%&gt;%</span> </span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">log_sales_price =</span> <span class="fu">log</span>(SalePrice),</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>         <span class="at">log_lot_area =</span> <span class="fu">log</span>(LotArea),</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>         <span class="at">neighbourhood =</span> <span class="fu">as.integer</span>(<span class="fu">as.factor</span>(Neighborhood)))</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A scatter plot shows a positive correlation between <code>log(SalePrice)</code> and <code>log(LotArea)</code>. Fitting OLS on the logarithms of both variables assumes a linear relationship on the multiplicative scale. All else equal, property prices tend to be higher with larger lot sizes. However, this univariate linear model clearly underfits the data and there are almost surely unobserved confounding variables.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> log_lot_area, <span class="at">y =</span> log_sales_price)) <span class="sc">+</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">colour =</span> <span class="st">'blue'</span>) <span class="sc">+</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">formula =</span> <span class="st">'y ~ x'</span>) <span class="sc">+</span> </span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggsave</span>(<span class="st">'figures/2r_pooling_scatter.png'</span>, <span class="at">dpi =</span> <span class="dv">300</span>, <span class="at">width=</span><span class="dv">10</span>, <span class="at">height =</span> <span class="dv">8</span>, <span class="at">units =</span> <span class="st">'in'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/2r_pooling_scatter.svg" class="img-fluid" style="width:80.0%">
</center>
<p>A potential reason for underfitting may be some neighbourhoods have higher average prices than other neighbourhoods (which would result in different intercepts). Furthermore, the <em>association</em> between housing prices and lot size may depend on different neighbourhoods as well (varying slopes). This variation could be driven by different zonings or housing densities within neighbourhoods that could impact the relationship between lot size and prices. Splitting the plot out by neighbourhood displays the heterogeneity in linear trends.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> log_lot_area, <span class="at">y =</span> log_sales_price)) <span class="sc">+</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">colour =</span> <span class="st">'blue'</span>) <span class="sc">+</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> lm, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">formula =</span> <span class="st">'y ~ x'</span>, <span class="at">fullrange =</span> <span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>Neighborhood) <span class="sc">+</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">strip.background =</span> <span class="fu">element_blank</span>())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/3r_facet_scatter.svg" class="img-fluid">
</center>
<p>We can see variation in the slopes and intercepts as well as imbalanced sampling between neighbourhood clusters. This and other unobserved confounders probably contributed to some of the weak/negative gradients. The small sample sizes in some neighbourhoods will be prone to overfitting and will give noisy estimates which will require regularisation.</p>
</section>
<section id="write-out-full-probability-model-1" class="level3">
<h3 class="anchored" data-anchor-id="write-out-full-probability-model-1">2) Write out full probability model</h3>
<p>3 basic linear models can be used to approach this problem:</p>
<ol type="1">
<li>Pooled OLS (assumes all observations come from “one neighbourhood”, equivalent to the OLS model in the first scatterplot)</li>
<li>No pooling OLS (conceptually the same as a dummy variable regression - assumes independence between all neighbourhoods)</li>
<li>Saturated regression (adds interactive effects between <code>log(LotArea)_i</code> and <code>neighbourhood</code> to no pooling OLS)</li>
</ol>
<p>I will use no pooling OLS to demonstrate the rest of the workflow. There is definitely room for improving these models. In fact, this problem is a good candidate for multilevel models. They allow for information to be borrowed between neighbourhood clusters through partial pooling and removes the assumption of independence. This will help regularise the effects of small and imbalanced sample sizes across <code>neighbourhood</code>. I will apply the full workflow using multilevel models in the next post.</p>
</section>
<section id="model-specification-1" class="level3">
<h3 class="anchored" data-anchor-id="model-specification-1">Model specification</h3>
<p>The no pooling regression is written out below, where <span class="math inline">\(i\)</span> indexes the property and <span class="math inline">\(j\)</span> indexes each neighbourhood. I’ve assigned a gaussian likelihood which assumes that the residuals are normally distributed.</p>
<p><span class="math display">\[
y_i \sim Normal(\mu_i, \sigma) \\
\\
\mu_i = \alpha_{j} + \beta * x_i \\
\]</span> Where <span class="math inline">\(y_i\)</span> is <code>log(SalesPrice)</code> and <span class="math inline">\(x_i\)</span> is <code>log(LotArea)</code> scaled to mean 0 and standard deviation 1. <span class="math inline">\(\alpha_j\)</span> is an intercept parameter for the jth neighbourhood in the sample. The slope coefficient can be interpreted as: a one standard deviation increase in <code>log(LotArea)</code> is a <span class="math inline">\(\beta\)</span> standard deviation change in <code>log(SalesPrice)</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>df  <span class="ot">&lt;-</span>  df <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">log_lot_area_z =</span> <span class="fu">scale</span>(log_lot_area),</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>                    <span class="at">log_sales_price_z =</span> <span class="fu">scale</span>(log_sales_price))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><span class="math display">\[
y_i = \frac{log(SalesPrice)_i - \overline{log(SalesPrice)}}{\sigma_{log(SalesPrice)}} \\
x_i = \frac{log(LotArea)_i - \overline{log(LotArea)}}{\sigma_{log(LotArea)}}
\]</span> Standardising both outcome and predictor variables makes sampling from the posterior distribution easier when we fit the model. If we had more continuous regressors, we could also compare the parameters on the same scale. Standardising also plays an important role in setting priors as we’ll see below.</p>
</section>
<section id="selecting-priors-1" class="level3">
<h3 class="anchored" data-anchor-id="selecting-priors-1">Selecting priors</h3>
<p>Probability distributions need to be assigned to the parameters for this to be a bayesian model. Setting priors is an opportunity to encode domain knowledge or results from related studies into the model. Unfortunately, I do not have much domain expertise or information about the context of this dataset to give very informative priors. So I have chosen to use weakly informative priors following the advice of the <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">Stan developers</a>. This will help me regularise model predictions within the plausible outcome space.</p>
<p>For <span class="math inline">\(\beta\)</span> I’ll assign a <span class="math inline">\(Normal(0, 1)\)</span> which puts ~95% of the probability between two standard deviations for a unit increase in <span class="math inline">\(x\)</span>. We want to hedge against overfitting by shrinking the coefficient towards zero. This is achieved by putting probability mass on all plausible values of <span class="math inline">\(\beta\)</span> with less weight on extreme relationships.</p>
<p><span class="math inline">\(\alpha_j\)</span> is the intercept for the <span class="math inline">\(j^{th}\)</span> neighbourhood. In a pooled OLS regression between price and lot area, the intercept <span class="math inline">\(\alpha\)</span> (ignoring the neighbourhood means ignoring the j subscript) would be interpreted as the value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x\)</span> is 0. Since <span class="math inline">\(x\)</span> has a mean of zero, <span class="math inline">\(\alpha\)</span> has the additional interpretation as the value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x\)</span> is equal to its sample mean. By construction, <span class="math inline">\(\alpha\)</span> must be 0, the sample mean of <span class="math inline">\(y\)</span>.</p>
<p>So in the case of <span class="math inline">\(\alpha_j\)</span> I set a normal prior with a mean of 0 and a standard deviation of 1 for all neighbourhoods, regularising neighbourhood effects within two standard deviations of the grand mean of <span class="math inline">\(y\)</span>.</p>
<p>The variance parameter <span class="math inline">\(\sigma\)</span> is defined over positive real numbers. So our prior should only put probabilistic weight on positive values. In this case I’ve chosen a weakly regularising <span class="math inline">\(exponential(1)\)</span> prior. Other candidate priors are the Half-Cauchy distribution or the Half-Normal which has thinner tails.</p>
<p>These weakly informative priors express my belief that the parameters of this model would overfit the sample and that we need to regularise their effects. Standardising the variables made this job much easier and intuitive. All together the full model looks like:</p>
<p><span class="math display">\[
y_i \sim Normal(\mu_i, \sigma) \\
\mu_i = \alpha_{j} + \beta * x_i \\
\alpha_j\sim Normal(0, 1)\\
\beta\sim Normal(0, 1) \\
\sigma\sim exp(1)
\]</span></p>
</section>
<section id="prior-predictive-checks---simulate-fake-data-from-the-implied-generative-model" class="level3">
<h3 class="anchored" data-anchor-id="prior-predictive-checks---simulate-fake-data-from-the-implied-generative-model">3) Prior predictive checks - simulate fake data from the implied generative model</h3>
<p>Prior predictive checks are useful for understanding the implications of our priors. Parameters are simulated from the joint prior distribution and visualised to see the implied relationships between the target and predictor variables. This will help diagnose any problems with our assumptions and modelling decisions. These checks become more important for generalised linear models since the outcome and parameter space are different due to the link function. For example, priors on parameters in a logistic regression are in the <em>log-odds space</em> and may behave differently to our expectations on the <em>outcome space</em>.</p>
<p>The code below includes all the inputs necessary to estimate the model on the data. Setting <code>run_estimation = 0</code> means Stan will only simulate values from the joint prior distribution since the likelihood is not evaluated (thanks to Jim for this handy <a href="https://khakieconomics.github.io/2017/04/30/An-easy-way-to-simulate-fake-data-in-stan.html">tip</a>).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>no_pooling_stan_code <span class="ot">=</span> <span class="st">"</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="st">// No pooling model for predicting housing prices</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="st">data {</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="st">    // Fitting the model on training data</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="st">    int&lt;lower=0&gt; N; // Number of rows</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="st">    int&lt;lower=0&gt; neighbourhood[N]; // neighbourhood categorical variable</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="st">    int&lt;lower=0&gt; N_neighbourhood; // number of neighbourhood categories</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="st">    vector[N] log_sales_price; // log sales price</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="st">    vector[N] log_lot_area; // log lot area</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="st">    // Adjust scale parameters in python</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="st">    real alpha_sd;</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="st">    real beta_sd;</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="st">    // Set to zero for prior predictive checks, set to one to evaluate likelihood</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a><span class="st">    int&lt;lower = 0, upper = 1&gt; run_estimation;</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a><span class="st">parameters {</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a><span class="st">    vector[N_neighbourhood] alpha; // Vector of alpha coefficients for each neighbourhood</span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="st">    real beta;</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a><span class="st">    real&lt;lower=0&gt; sigma;</span></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a><span class="st">model {</span></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a><span class="st">    // Priors</span></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a><span class="st">    target += normal_lpdf(alpha | 0, alpha_sd);</span></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a><span class="st">    target += normal_lpdf(beta | 0, beta_sd);</span></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a><span class="st">    target += exponential_lpdf(sigma |1);</span></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a><span class="st">    //target += normal_lpdf(sigma |0, 1);</span></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a><span class="st">    // Likelihood</span></span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a><span class="st">    if(run_estimation==1){</span></span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a><span class="st">        target += normal_lpdf(log_sales_price | alpha[neighbourhood] + beta * log_lot_area, sigma);</span></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a><span class="st">    }</span></span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a><span class="st">generated quantities {</span></span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a><span class="st">    // Uses fitted model to generate values of interest without re running the sampler</span></span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a><span class="st">    vector[N] log_lik; // Log likelihood</span></span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a><span class="st">    vector[N] y_hat; // Predictions using training data</span></span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a><span class="st">    {</span></span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a><span class="st">    for(n in 1:N){</span></span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a><span class="st">          log_lik[n] = normal_lpdf(log_sales_price | alpha[neighbourhood[n]] + beta * log_lot_area[n], sigma);</span></span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a><span class="st">          y_hat[n] = normal_rng(alpha[neighbourhood[n]] + beta * log_lot_area[n], sigma);      </span></span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a><span class="st">        }</span></span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a><span class="st">    }</span></span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a><span class="st">"</span></span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a><span class="co"># List contains all data inputs</span></span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a>npm_data_check <span class="ot">=</span> <span class="fu">list</span>(<span class="at">N =</span> <span class="fu">nrow</span>(df),</span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a>                      <span class="at">log_sales_price =</span> <span class="fu">as.vector</span>(df<span class="sc">$</span>log_sales_price_z),</span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a>                      <span class="at">log_lot_area =</span> <span class="fu">as.vector</span>(df<span class="sc">$</span>log_lot_area_z),</span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a>                      <span class="at">neighbourhood =</span> <span class="fu">as.vector</span>(df<span class="sc">$</span>neighbourhood),</span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a>                      <span class="at">N_neighbourhood =</span> <span class="fu">max</span>(df<span class="sc">$</span>neighbourhood),</span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true" tabindex="-1"></a>                      <span class="at">alpha_sd =</span> <span class="dv">1</span>, </span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true" tabindex="-1"></a>                      <span class="at">beta_sd =</span> <span class="dv">1</span>, </span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true" tabindex="-1"></a>                      <span class="at">run_estimation =</span> <span class="dv">0</span>)</span>
<span id="cb21-58"><a href="#cb21-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-59"><a href="#cb21-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw samples from joint prior distribution</span></span>
<span id="cb21-60"><a href="#cb21-60" aria-hidden="true" tabindex="-1"></a>fit_npm_check <span class="ot">=</span> <span class="fu">stan</span>(<span class="at">model_code =</span> no_pooling_stan_code, <span class="at">data =</span> npm_data_check, <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">seed =</span> <span class="dv">12345</span>)</span>
<span id="cb21-61"><a href="#cb21-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-62"><a href="#cb21-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract samples into a pandas dataframe</span></span>
<span id="cb21-63"><a href="#cb21-63" aria-hidden="true" tabindex="-1"></a>npm_df_check <span class="ot">=</span> <span class="fu">as.data.frame</span>(fit_npm_check)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<blockquote class="blockquote">
<p>For the prior predictive checks, we recommend not cleaving too closely to the observed data and instead aiming for a prior data generating process that can produce plausible data sets, not necessarily ones that are indistinguishable from observed data. - <a href="https://arxiv.org/abs/1709.01449">Visualisation Bayesian Workflow - Gabry, Simpson, Vehtari, Betancourt, Gelman (2019)</a></p>
</blockquote>
<p>The implied predictions of our priors are visualised below. I’ve arbitrarily chosen the 4th neighbourhood index (<span class="math inline">\(\alpha_{j=4}\)</span>) since the priors for the neighbourhoods are the same. Weakly informative priors should create bounds between possible values while allowing for some implausible relationships. Remembering that 95% of gaussian mass exists within two standard deviations of the mean is a useful guide for determining what is reasonable.</p>
<p>Let’s see an example of setting uninformative priors and its implications of the data generating process. I’ve set the scale parameters for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> to be 10 which are quite diffuse. The implied predictions of the mean are much wider and well beyond the minimum and maximum values in the real data. This suggests that the model is giving too much probabilistic weight to highly implausible datasets.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model with diffuse priors</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>npm_data_check_wide <span class="ot">=</span> <span class="fu">list</span>(<span class="at">N =</span> <span class="fu">nrow</span>(df),</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>                      <span class="at">log_sales_price =</span> <span class="fu">as.vector</span>(df<span class="sc">$</span>log_sales_price_z),</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>                      <span class="at">log_lot_area =</span> <span class="fu">as.vector</span>(df<span class="sc">$</span>log_lot_area_z),</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>                      <span class="at">neighbourhood =</span> <span class="fu">as.vector</span>(df<span class="sc">$</span>neighbourhood),</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>                      <span class="at">N_neighbourhood =</span> <span class="fu">max</span>(df<span class="sc">$</span>neighbourhood),</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>                      <span class="at">alpha_sd =</span> <span class="dv">10</span>, </span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>                      <span class="at">beta_sd =</span> <span class="dv">10</span>, </span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>                      <span class="at">run_estimation =</span> <span class="dv">0</span>)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>fit_npm_check_wide <span class="ot">=</span> <span class="fu">stan</span>(<span class="at">model_code =</span> no_pooling_stan_code, <span class="at">data=</span>npm_data_check_wide, <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">seed =</span> <span class="dv">12345</span>)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>npm_df_check_wide <span class="ot">=</span> <span class="fu">as.data.frame</span>(fit_npm_check_wide)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create length of std x variables</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="sc">-</span><span class="dv">3</span>, <span class="at">to =</span> <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create empty dataframe and fill it with parameters</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>df_wide <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">matrix</span>(<span class="at">ncol=</span><span class="dv">100</span>, <span class="at">nrow=</span><span class="dv">200</span>))</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>) {</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>  alpha <span class="ot">&lt;-</span> npm_df_check_wide<span class="sc">$</span><span class="st">`</span><span class="at">alpha[4]</span><span class="st">`</span>[i]</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>  beta <span class="ot">&lt;-</span> npm_df_check_wide<span class="sc">$</span>beta[i]</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>  df_wide[, i] <span class="ot">&lt;-</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> x</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Tidy up filled dataframe</span></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>df_wide <span class="ot">&lt;-</span> df_wide <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">x =</span> x) <span class="sc">%&gt;%</span> <span class="fu">pivot_longer</span>(<span class="fu">starts_with</span>(<span class="st">"V"</span>))</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_wide, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> value)) <span class="sc">+</span></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> name), <span class="at">size =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'Prior predictive checks -- Uninformative (flat) priors'</span>,</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>             <span class="at">x =</span> <span class="st">'x (z-scores)'</span>,</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>             <span class="at">y =</span> <span class="st">'Fitted y (z_scores)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/5r_prior_predictive_check_wide.svg" class="img-fluid">
</center>
<p>Our original scale parameters of 1 produce more reasonable relationships. There are still some extreme regression lines implied by our data generating process, but they are bound to more realistic outcomes relative to the diffuse priors.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create length of std x variables</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="sc">-</span><span class="dv">3</span>, <span class="at">to =</span> <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create empty dataframe and fill it with parameters</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>df_regularising <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">matrix</span>(<span class="at">ncol=</span><span class="dv">100</span>, <span class="at">nrow=</span><span class="dv">200</span>))</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>) {</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>  alpha <span class="ot">&lt;-</span> npm_df_check<span class="sc">$</span><span class="st">`</span><span class="at">alpha[4]</span><span class="st">`</span>[i]</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>  beta <span class="ot">&lt;-</span> npm_df_check<span class="sc">$</span>beta[i]</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>  df_regularising[, i] <span class="ot">&lt;-</span> alpha <span class="sc">+</span> beta <span class="sc">*</span> x</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Tidy up filled dataframe</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>df_regularising <span class="ot">&lt;-</span> df_regularising <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">x =</span> x) <span class="sc">%&gt;%</span> <span class="fu">pivot_longer</span>(<span class="fu">starts_with</span>(<span class="st">"V"</span>))</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_regularising, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> value)) <span class="sc">+</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> name), <span class="at">size =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'Prior predictive checks -- Weakly regularizing priors'</span>,</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>             <span class="at">x =</span> <span class="st">'x (z-scores)'</span>,</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>             <span class="at">y =</span> <span class="st">'Fitted y (z_scores)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/4r_prior_predictive_check.svg" class="img-fluid">
</center>
<p>Putting both sets of lines on the same scale emphasises the difference in simulated values. The blue lines from the previous graph cover a tighter space relative to the simulations from the uninformative priors.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_wide, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> value)) <span class="sc">+</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> name), <span class="at">size =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> df_regularising, <span class="fu">aes</span>(<span class="at">group =</span> name), <span class="at">size =</span> <span class="fl">0.2</span>, <span class="at">colour =</span> <span class="st">'blue'</span>) <span class="sc">+</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'Prior predictive checks -- Uninformative (flat) priors'</span>,</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">x =</span> <span class="st">'x (z-scores)'</span>,</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>             <span class="at">y =</span> <span class="st">'Fitted y (z_scores)'</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/5r_prior_predictive_check_compare.svg" class="img-fluid">
</center>
</section>
<section id="fit-model-on-fake-data-1" class="level3">
<h3 class="anchored" data-anchor-id="fit-model-on-fake-data-1">4) Fit model on fake data</h3>
<p>We can use the simulations to see if our model can successfully estimate the parameters used to generate fake data (the implied <span class="math inline">\(\hat{y}\)</span>). Take a draw from the prior samples (e.g.&nbsp;the 50th simulation) and estimate the model on the data produced by these parameters. Let’s see if the model fitted on fake data can capture the “true” parameters (dotted red lines) of the data generating process. If the model cannot capture the <em>known</em> parameters which generated fake data, there is no certainty it will be estimating the correct parameters on real data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pick random simulation, let's say 50</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>random_draw <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the simulated (fake) data implied by the parameters in sample 50</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>y_sim <span class="ot">&lt;-</span>  npm_df_check[random_draw, ] <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="fu">contains</span>(<span class="st">'y_hat'</span>)) <span class="sc">%&gt;%</span> <span class="fu">t</span>()</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the parameters corresponding to sample 50</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>true_parameters <span class="ot">=</span> npm_df_check[random_draw,] <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="fu">contains</span>(<span class="fu">c</span>(<span class="st">'alpha'</span>,<span class="st">'beta'</span>,<span class="st">'sigma'</span>)))</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="co"># List contains all data inputs</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>npm_data_check_ <span class="ot">=</span> <span class="fu">list</span>(<span class="at">N =</span> <span class="fu">nrow</span>(df),</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>                      <span class="at">log_sales_price =</span> <span class="fu">as.vector</span>(y_sim), <span class="co"># target is now extracted fake data in sample 50</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>                      <span class="at">log_lot_area =</span> <span class="fu">as.vector</span>(df<span class="sc">$</span>log_lot_area_z),</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>                      <span class="at">neighbourhood =</span> <span class="fu">as.vector</span>(df<span class="sc">$</span>neighbourhood),</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>                      <span class="at">N_neighbourhood =</span> <span class="fu">max</span>(df<span class="sc">$</span>neighbourhood),</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>                      <span class="at">alpha_sd =</span> <span class="dv">1</span>, </span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>                      <span class="at">beta_sd =</span> <span class="dv">1</span>, </span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>                      <span class="at">run_estimation =</span> <span class="dv">1</span>)</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model on the fake data</span></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>fit_npm_check_ <span class="ot">=</span> <span class="fu">stan</span>(<span class="at">model_code =</span> no_pooling_stan_code, <span class="at">data=</span>npm_data_check_, <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">seed =</span> <span class="dv">12345</span>)</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>npm_df_check_ <span class="ot">=</span> <span class="fu">as.data.frame</span>(fit_npm_check_)</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract parameters and tidy dataframe</span></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>fake_fit <span class="ot">=</span> npm_df_check_ <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="fu">contains</span>(<span class="fu">c</span>(<span class="st">'alpha'</span>, <span class="st">'beta'</span>, <span class="st">'sigma'</span>)))</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>parameter_df <span class="ot">=</span> fake_fit <span class="sc">%&gt;%</span> <span class="fu">pivot_longer</span>(<span class="fu">everything</span>()) <span class="sc">%&gt;%</span> <span class="fu">rename</span>(<span class="at">parameters =</span> name)</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>parameter_df<span class="sc">$</span>parameters <span class="ot">&lt;-</span> <span class="fu">factor</span>(parameter_df<span class="sc">$</span>parameters, <span class="at">levels =</span> (parameter_df<span class="sc">$</span>parameters <span class="sc">%&gt;%</span> <span class="fu">unique</span>()))</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot will give distributions of all parameters to see if it can capture the known parameters</span></span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(parameter_df, <span class="fu">aes</span>(value)) <span class="sc">+</span> </span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">colour =</span> <span class="st">'blue'</span>) <span class="sc">+</span> </span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>parameters, <span class="at">scales =</span> <span class="st">'free'</span>) <span class="sc">+</span> </span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">data =</span> (true_parameters <span class="sc">%&gt;%</span> <span class="fu">pivot_longer</span>(<span class="fu">everything</span>()) <span class="sc">%&gt;%</span> <span class="fu">rename</span>(<span class="at">parameters =</span> name)), <span class="fu">aes</span>(<span class="at">xintercept =</span> value), <span class="at">colour =</span> <span class="st">'red'</span>) <span class="sc">+</span> </span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'Model Checking - red lines are "true" parameters'</span>,</span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">''</span>) <span class="sc">+</span> </span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">strip.background =</span> <span class="fu">element_blank</span>()) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/6r_fit_fake_data.svg" class="img-fluid">
</center>
</section>
<section id="estimate-model-on-real-data-1" class="level3">
<h3 class="anchored" data-anchor-id="estimate-model-on-real-data-1">5) Estimate model on real data</h3>
<p>Set <code>run_estimation=1</code> and run the code to fit the model. Stan will sample the joint posterior distribution using the default Markov chain Monte Carlo (MCMC) algorithm, the <a href="https://mc-stan.org/docs/2_18/reference-manual/hmc-algorithm-parameters.html">No-U-Turn sampler (NUTs)</a>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dictionary with data inputs - set run_estimation=1</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>npm_data <span class="ot">=</span> <span class="fu">list</span>(<span class="at">N =</span> <span class="fu">nrow</span>(df),</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>                      <span class="at">log_sales_price =</span> <span class="fu">as.vector</span>(df<span class="sc">$</span>log_sales_price_z),</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>                      <span class="at">log_lot_area =</span> <span class="fu">as.vector</span>(df<span class="sc">$</span>log_lot_area_z),</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>                      <span class="at">neighbourhood =</span> <span class="fu">as.vector</span>(df<span class="sc">$</span>neighbourhood),</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>                      <span class="at">N_neighbourhood =</span> <span class="fu">max</span>(df<span class="sc">$</span>neighbourhood),</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>                      <span class="at">alpha_sd =</span> <span class="dv">1</span>, </span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>                      <span class="at">beta_sd =</span> <span class="dv">1</span>, </span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>                      <span class="at">run_estimation =</span> <span class="dv">1</span>)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model by sampling from posterior distribution</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>fit_npm <span class="ot">=</span> <span class="fu">stan</span>(<span class="at">model_code =</span> no_pooling_stan_code, <span class="at">data =</span> npm_data, <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">seed =</span> <span class="dv">12345</span>)</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract samples into dataframe</span></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>fit_npm_df <span class="ot">=</span> <span class="fu">as.data.frame</span>(fit_npm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="check-whether-mcmc-sampler-and-model-fit-1" class="level3">
<h3 class="anchored" data-anchor-id="check-whether-mcmc-sampler-and-model-fit-1">6) Check whether MCMC sampler and model fit<!--{.tabset .tabset-fade .tabset-pills}--></h3>
<p>Stan won’t have trouble sampling from such a simple model, so I won’t go through chain diagnostics in detail. I’ve included number of effective samples and Rhat diagnostics for completeness. We can see the posterior distributions of all the parameters by looking at the traceplot as well.</p>
<section id="traceplot-1" class="level4">
<h4 class="anchored" data-anchor-id="traceplot-1">Traceplot</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect model fit</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="fu">color_scheme_set</span>(<span class="st">"mix-blue-red"</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="fu">mcmc_combo</span>(</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">as.array</span>(fit_npm),</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a> <span class="at">combo =</span> <span class="fu">c</span>(<span class="st">"dens_overlay"</span>, <span class="st">"trace"</span>),</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a> <span class="at">pars =</span> <span class="fu">c</span>(<span class="st">'alpha[1]'</span>, <span class="st">'beta'</span>, <span class="st">'sigma'</span>),</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a> <span class="at">gg_theme =</span> <span class="fu">legend_none</span>()) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/7r_traceplot.svg" class="img-fluid">
</center>
</section>
<section id="posterior-distributions-1" class="level4">
<h4 class="anchored" data-anchor-id="posterior-distributions-1">Posterior distributions</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">stan_plot</span>(fit_npm, </span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>          <span class="at">show_density =</span> <span class="cn">FALSE</span>, </span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>          <span class="at">unconstrain =</span> <span class="cn">TRUE</span>, </span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>          <span class="at">pars =</span> <span class="fu">c</span>(<span class="st">'alpha'</span>, <span class="st">'beta'</span>, <span class="st">'sigma'</span>)) <span class="sc">+</span> </span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'Posterior distributions of fitted parameters'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/7r_posterior.svg" class="img-fluid">
</center>
</section>
<section id="neff-rhat-1" class="level4">
<h4 class="anchored" data-anchor-id="neff-rhat-1">neff / Rhat</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(fit_npm, <span class="at">pars =</span> <span class="fu">c</span>(<span class="st">'alpha'</span>, <span class="st">'beta'</span>, <span class="st">'sigma'</span>), </span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>                         <span class="at">probs=</span><span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.50</span>, <span class="fl">0.975</span>), </span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>                         <span class="at">digits_summary=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<pre><code>Inference for Stan model: 2be0e54fe1314f469f9b784aa4444aba.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

            mean se_mean    sd   2.5%    50%  97.5% n_eff  Rhat
alpha[1]   1.005   0.002 0.148  0.708  1.007  1.291  5778 1.000
alpha[2]   0.565   0.005 0.395 -0.218  0.567  1.348  7517 1.000
alpha[3]  -0.099   0.002 0.165 -0.426 -0.099  0.221  5493 1.000
alpha[4]  -0.687   0.001 0.083 -0.855 -0.686 -0.523  7361 1.000
alpha[5]   0.001   0.001 0.122 -0.237  0.001  0.241  6665 0.999
alpha[6]   0.330   0.000 0.050  0.233  0.330  0.426 10574 0.999
alpha[7]   0.340   0.001 0.086  0.175  0.340  0.506  7469 0.999
alpha[8]  -0.777   0.001 0.061 -0.897 -0.776 -0.656  7542 0.999
alpha[9]   0.216   0.001 0.071  0.073  0.216  0.355  7027 1.000
alpha[10] -1.331   0.001 0.098 -1.524 -1.330 -1.136  8231 0.999
alpha[11] -0.406   0.002 0.151 -0.705 -0.406 -0.100  5802 0.999
alpha[12] -0.320   0.001 0.085 -0.488 -0.321 -0.153  7145 1.000
alpha[13] -0.440   0.000 0.041 -0.519 -0.442 -0.361  7392 1.000
alpha[14]  1.369   0.001 0.096  1.177  1.369  1.559  7084 0.999
alpha[15]  0.315   0.002 0.199 -0.083  0.315  0.704  7174 1.000
alpha[16]  1.412   0.001 0.070  1.274  1.412  1.548  6393 0.999
alpha[17]  0.100   0.001 0.071 -0.039  0.099  0.244  7724 0.999
alpha[18] -0.684   0.001 0.057 -0.797 -0.683 -0.574  8749 1.000
alpha[19] -0.596   0.001 0.069 -0.730 -0.596 -0.461  6336 0.999
alpha[20]  0.121   0.001 0.079 -0.036  0.122  0.282  7308 1.000
alpha[21]  0.869   0.001 0.067  0.738  0.869  1.002  6553 1.000
alpha[22]  1.422   0.001 0.122  1.181  1.421  1.667  7990 1.000
alpha[23] -0.357   0.001 0.121 -0.586 -0.357 -0.122  7743 0.999
alpha[24]  0.502   0.001 0.099  0.311  0.501  0.692  7553 0.999
alpha[25]  0.517   0.002 0.181  0.166  0.516  0.881  8580 0.999
beta       0.348   0.000 0.021  0.307  0.348  0.389  3651 1.000
sigma      0.607   0.000 0.011  0.585  0.607  0.630  8275 1.000

Samples were drawn using NUTS(diag_e) at Thu Nov 12 11:58:44 2020.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
</section>
</section>
<section id="posterior-predictive-check-to-evaluate-model-fit" class="level3">
<h3 class="anchored" data-anchor-id="posterior-predictive-check-to-evaluate-model-fit">7) Posterior predictive check to evaluate model fit</h3>
<p>How well did the model perform? We can perform posterior predictive checks to see if sampled distributions from the fitted model can approximate the density of <code>SalesPrice</code>. If the model performs well, it should be able to retrodict the density of the data used to train the model. The blue lines are the predictions drawn from the joint posterior distribution compared with the observed density of the target <span class="math inline">\(y\)</span> variable.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Select 300 samples to plot against observed distribution</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="fu">color_scheme_set</span>(<span class="at">scheme =</span> <span class="st">"blue"</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>yrep <span class="ot">&lt;-</span> <span class="fu">extract</span>(fit_npm)[[<span class="st">"y_hat"</span>]]</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>samples <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(yrep), <span class="dv">300</span>)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ppc_dens_overlay</span>(<span class="fu">as.vector</span>(df<span class="sc">$</span>log_sales_price_z), yrep[samples, ])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/8r_posterior_predictive_check.png" class="img-fluid">
</center>
<p>Reversing the data transformations gives back the posterior predictive checks on the natural scale (rescale <span class="math inline">\(y\)</span> and exponentiate <code>log(SalesPrice)</code> to get back <code>SalesPrice</code>):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Take 300 samples of posterior predictive checks and revert back to natural scale</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>ppc <span class="ot">&lt;-</span> yrep[samples, ] <span class="sc">%&gt;%</span> </span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">t</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">apply</span>(., <span class="at">MARGIN  =</span> <span class="dv">2</span>, <span class="at">FUN =</span> <span class="cf">function</span>(x) <span class="fu">exp</span>((x <span class="sc">*</span> <span class="fu">sd</span>(df<span class="sc">$</span>log_sales_price)) <span class="sc">+</span> <span class="fu">mean</span>(df<span class="sc">$</span>log_sales_price))) <span class="sc">%&gt;%</span> </span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>() <span class="sc">%&gt;%</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="fu">everything</span>())</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot densities</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ppc, <span class="fu">aes</span>(value)) <span class="sc">+</span> </span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="fu">aes</span>(<span class="at">group =</span> name), <span class="at">colour =</span> <span class="st">"lightblue"</span>) <span class="sc">+</span> </span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">data =</span> (df <span class="sc">%&gt;%</span> <span class="fu">select</span>(SalePrice) <span class="sc">%&gt;%</span> <span class="fu">rename</span>(<span class="at">value =</span> SalePrice)), <span class="at">colour =</span> <span class="st">'black'</span>) <span class="sc">+</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">"none"</span>, <span class="at">axis.text.y=</span><span class="fu">element_blank</span>()) <span class="sc">+</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'Posterior predictive checks - Black: observed SalePrice</span><span class="sc">\n</span><span class="st">Light Blue: Posterior Samples'</span>) <span class="sc">+</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggsave</span>(<span class="st">'figures/9r_posterior_predictive_check_outcomescale.png'</span>, <span class="at">dpi =</span> <span class="dv">300</span>, <span class="at">height =</span> <span class="dv">6</span>, <span class="at">width =</span> <span class="dv">9</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<center>
<img src="figures/9r_posterior_predictive_check_outcomescale.png" class="img-fluid">
</center>
<p>Not bad for a simple model. There is definitely room for iteration and improvement.</p>
</section>
</div>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The last thing we should do is compare the fits of multiple models and evaluate their performance using cross validation for model selection. The next post applies the full workflow using multilevel models and compares performance using techniques such as Leave One Out - Cross Validation (LOO-CV). Model performance can also be evaluated on out of sample test data as well since this is a predictive task (Kaggle computes the log RMSE of the out of sample dataset).</p>
<p>This is not an exhaustive review of all the diagnostics and visualisations that can be performed in a workflow. There are many ways of evaluating model fit and diagnostics that could validate or invalidate the model. Below are a list of resources which give more detailed examples on various bayesian models and workflows:</p>
<ul>
<li><p><a href="https://mc-stan.org/users/documentation/case-studies">Stan case studies</a></p></li>
<li><p><a href="https://docs.pymc.io/nb_examples/index.html">PyMC3 examples</a></p></li>
<li><p>Michael Betancourt’s case study on a <a href="https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html#1_questioning_authority">Principled Bayesian Workflow</a> and all his other <a href="https://betanalpha.github.io/writing/">case studies</a></p></li>
<li><p><a href="https://arxiv.org/abs/2011.01808">Bayesian Workflow</a> and some links to the development of bayesian workflow over the past few years can be found <a href="https://statmodeling.stat.columbia.edu/2020/11/10/bayesian-workflow/">here</a></p></li>
<li><p><a href="https://mc-stan.org/users/documentation/case-studies/pystan_workflow.html">Robust Statistical Workflow with PyStan</a></p></li>
<li><p><a href="https://mc-stan.org/users/documentation/case-studies/rstan_workflow.html">Robust Statistical Workflow with RStan</a></p></li>
</ul>
<p>Notebooks that reproduce the models/plots/etc:</p>
<p><a href="https://github.com/bennywee/house_prices_kaggle/blob/master/blog/blog_code_python.ipynb">Python</a></p>
<p><a href="https://github.com/bennywee/house_prices_kaggle/blob/master/blog/blog_code_r.Rmd">R</a></p>
<p><a href="https://bennywee.github.io/">Return home</a></p>
</section>
<section id="original-computing-environment" class="level2">
<h2 class="anchored" data-anchor-id="original-computing-environment">Original Computing Environment</h2>
<pre><code>%load_ext watermark
%watermark -n -v -u -iv -w -a Benjamin_Wee

seaborn 0.11.0
pandas  1.1.3
arviz   0.10.0
pystan  2.19.0.0
numpy   1.19.1
Benjamin_Wee 
last updated: Thu Nov 19 2020 

CPython 3.6.12
IPython 5.8.0
watermark 2.0.2</code></pre>
<pre><code>sessionInfo()

R version 4.0.3 (2020-10-10)
Platform: x86_64-apple-darwin17.0 (64-bit)
Running under: macOS Mojave 10.14.6

Matrix products: default
BLAS:   /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib

Random number generation:
 RNG:     Mersenne-Twister 
 Normal:  Inversion 
 Sample:  Rounding 
 
locale:
[1] en_AU.UTF-8/en_AU.UTF-8/en_AU.UTF-8/C/en_AU.UTF-8/en_AU.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] gridExtra_2.3        gdtools_0.2.2        svglite_1.2.3.2      bayesplot_1.7.2      rstan_2.21.2        
 [6] StanHeaders_2.21.0-6 forcats_0.5.0        stringr_1.4.0        dplyr_1.0.2          purrr_0.3.4         
[11] readr_1.4.0          tidyr_1.1.2          tibble_3.0.4         ggplot2_3.3.2        tidyverse_1.3.0     

loaded via a namespace (and not attached):
 [1] httr_1.4.2         jsonlite_1.7.1     splines_4.0.3      modelr_0.1.8       RcppParallel_5.0.2 assertthat_0.2.1  
 [7] stats4_4.0.3       cellranger_1.1.0   yaml_2.2.1         pillar_1.4.6       backports_1.2.0    lattice_0.20-41   
[13] reticulate_1.18    glue_1.4.2         digest_0.6.27      rvest_0.3.6        colorspace_1.4-1   htmltools_0.5.0   
[19] Matrix_1.2-18      plyr_1.8.6         pkgconfig_2.0.3    broom_0.7.2        haven_2.3.1        scales_1.1.1      
[25] processx_3.4.4     mgcv_1.8-33        generics_0.1.0     farver_2.0.3       ellipsis_0.3.1     withr_2.3.0       
[31] cli_2.1.0          magrittr_1.5       crayon_1.3.4       readxl_1.3.1       evaluate_0.14      ps_1.4.0          
[37] fs_1.5.0           fansi_0.4.1        nlme_3.1-149       xml2_1.3.2         pkgbuild_1.1.0     tools_4.0.3       
[43] loo_2.3.1          prettyunits_1.1.1  hms_0.5.3          lifecycle_0.2.0    matrixStats_0.57.0 V8_3.4.0          
[49] munsell_0.5.0      reprex_0.3.0       callr_3.5.1        compiler_4.0.3     systemfonts_0.3.2  rlang_0.4.8       
[55] grid_4.0.3         ggridges_0.5.2     rstudioapi_0.11    labeling_0.4.2     rmarkdown_2.5      gtable_0.3.0      
[61] codetools_0.2-16   inline_0.3.16      DBI_1.1.0          curl_4.3           reshape2_1.4.4     R6_2.5.0          
[67] lubridate_1.7.9    knitr_1.30         utf8_1.1.4         stringi_1.5.3      parallel_4.0.3     Rcpp_1.0.5        
[73] vctrs_0.3.4        dbplyr_2.0.0       tidyselect_1.1.0   xfun_0.19         </code></pre>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>